{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>lueur is here to help you uncover and address resiliency issues early in your development cycle. By easily injecting network faults into your application\u2019s daily workflows, lueur encourages you to shift resiliency concerns to the left, long before reaching production. The result: more confident engineering teams and applications that gracefully handle the unexpected.</p> <p>At its core, lueur acts as a local proxy you can route traffic through, giving you fine-grained control over conditions like latency, jitter, and faults. Rather than waiting until late-stage testing or worse, customer reports, you can quickly see how your application responds when the network isn\u2019t perfect.</p> <p>Why lueur?</p> <ul> <li>Speed: Quickly stand up a local test environment with minimal setup.</li> <li>Simplicity: Just a few commands let you inject latency or run complex   scenarios\u2014no steep learning curve required.</li> <li>Extensibility: Tweak parameters, plug into automated tests, and integrate   with your existing CI/CD pipelines.</li> <li>Insight: Generate structured reports that help pinpoint issues and   identify ways to improve resiliency.</li> </ul> <p>Take a look at the Tutorials, How-To Guides, Explanations, and Reference sections to start exploring what lueur can do for your team.</p> <ul> <li>:fontawesome-brands-html5: HTML for content and structure</li> <li>:fontawesome-brands-js: JavaScript for interactivity</li> <li>:fontawesome-brands-css3: CSS for text running out of boxes</li> <li>:fontawesome-brands-internet-explorer: Internet Explorer ... huh?</li> </ul>"},{"location":"explanations/fault-injection-basics/","title":"Understanding Fault Injection: Purpose and Use in Reliability Engineering","text":"<p>Fault injection isn\u2019t just about breaking things on purpose\u2014it's a proactive strategy to uncover hidden weaknesses in your system before they become critical issues in production. This page explains the purpose behind different types of network faults and how engineers can use them to improve overall reliability.</p>"},{"location":"explanations/fault-injection-basics/#why-inject-faults","title":"Why Inject Faults?","text":"<p>In production, systems rarely operate under ideal conditions. Network delays, transient errors, and unexpected outages can occur at any time. By intentionally simulating these adverse conditions during development, you can:</p> <ul> <li> <p>Uncover Hidden Weaknesses:   Identify parts of your system that are sensitive to delays or errors before they cause outages in real-world scenarios.</p> </li> <li> <p>Validate Resilience Strategies:   Test whether your retry mechanisms, circuit breakers, or fallback procedures are effective in mitigating issues when faults occur.</p> </li> <li> <p>Enhance User Experience:   Ensure that even under degraded conditions, your application remains responsive and provides meaningful feedback to end users.</p> </li> <li> <p>Promote Proactive Improvement:   Foster a culture of reliability-first development, where engineers routinely stress-test their systems and refine them based on measurable outcomes.</p> </li> </ul>"},{"location":"explanations/fault-injection-basics/#types-of-faults-and-their-purposes","title":"Types of Faults and Their Purposes","text":"<p>Each fault type has a distinct role in helping you simulate and analyze adverse network conditions:</p>"},{"location":"explanations/fault-injection-basics/#latency-faults","title":"Latency Faults","text":"<ul> <li>Purpose:   To simulate delays in network communication.  </li> <li>Use Case:   Assess how increased response times affect user experience and trigger timeouts or slowdowns in your application.  </li> <li>Engineering Focus:   Fine-tune timeout settings, optimize service interactions, and improve caching strategies.</li> </ul>"},{"location":"explanations/fault-injection-basics/#packet-loss-faults","title":"Packet Loss Faults","text":"<ul> <li>Purpose:   To emulate conditions where data packets are dropped during transmission.  </li> <li>Use Case:   Evaluate the robustness of retransmission logic, error correction, and fallback mechanisms in your application.  </li> <li>Engineering Focus:   Enhance network reliability and ensure graceful degradation when parts of the data fail to arrive.</li> </ul>"},{"location":"explanations/fault-injection-basics/#bandwidth-faults","title":"Bandwidth Faults","text":"<ul> <li>Purpose:   To mimic limited network capacity by throttling data transfer rates.  </li> <li>Use Case:   Determine how well your application performs when network speed is constrained, affecting download/upload times.  </li> <li>Engineering Focus:   Optimize data compression, prioritize critical data flows, and adjust streaming or bulk data transfers.</li> </ul>"},{"location":"explanations/fault-injection-basics/#jitter-faults","title":"Jitter Faults","text":"<ul> <li>Purpose:   To simulate the variability in delay (jitter) that occurs in real-world networks.  </li> <li>Use Case:   Test the consistency of your service under fluctuating network conditions where delays are not uniform.  </li> <li>Engineering Focus:   Smooth out performance variations by refining buffering strategies and adaptive rate controls.</li> </ul>"},{"location":"explanations/fault-injection-basics/#dns-faults","title":"DNS Faults","text":"<ul> <li>Purpose:   To mimic issues in domain name resolution, such as slow or failed lookups.  </li> <li>Use Case:   Check how delays or failures in DNS resolution impact your application\u2019s ability to connect to services.  </li> <li>Engineering Focus:   Implement caching for DNS queries and design robust fallbacks for name resolution failures.</li> </ul>"},{"location":"explanations/fault-injection-basics/#http-error-faults","title":"HTTP Error Faults","text":"<ul> <li>Purpose:   To introduce server-side errors (like HTTP 500 or 404) into your workflow.  </li> <li>Use Case:   Ensure that your application gracefully handles unexpected errors from upstream services.  </li> <li>Engineering Focus:   Strengthen error-handling routines, validate user-friendly error messages, and implement effective retry or fallback mechanisms.</li> </ul>"},{"location":"explanations/fault-injection-basics/#combining-fault-injection-with-different-distributions","title":"Combining Fault Injection with Different Distributions","text":"<p>Beyond the fault types, how you model the injection of these faults is equally important:</p> <ul> <li> <p>Uniform Distribution:   Simulates a consistent and predictable range of delays or errors.</p> </li> <li> <p>Normal Distribution:   Reflects real-world conditions where most delays cluster around an average value with few extremes.</p> </li> <li> <p>Pareto Distribution:   Mimics scenarios with rare but severe latency spikes or errors.</p> </li> <li> <p>Pareto Normal Distribution:   Offers a hybrid approach where typical conditions are normal but with occasional heavy-tailed anomalies.</p> </li> </ul> <p>By choosing the right fault type and distribution model, you can tailor your testing to match realistic conditions that your system might encounter. This granular approach lets you focus on specific reliability challenges and iteratively improve your system\u2019s resilience.</p>"},{"location":"explanations/fault-injection-basics/#in-summary","title":"In Summary","text":"<p>Fault injection is a powerful tool in your reliability engineering toolkit. It not only helps you detect vulnerabilities but also guides you in making informed improvements. By understanding the purpose behind each fault type and how to apply different distribution models, you can build robust systems that continue to perform\u2014even when the network isn\u2019t perfect.</p> <p>Embrace fault injection as a regular part of your development cycle, and transform unexpected failures into opportunities for building better, more resilient software.</p>"},{"location":"explanations/understanding-ebpf/","title":"Understanding eBPF and its Context In Reliability Engineering","text":"<p>eBPF (extended Berkeley Packet Filter) is a powerful, flexible technology built into the Linux kernel. It allows developers to run custom programs safely and efficiently in kernel space. In the context of reliability engineering, eBPF opens up new possibilities for monitoring, tracing, and manipulating network traffic\u2014without having to modify your application or its configuration.</p>"},{"location":"explanations/understanding-ebpf/#ebpf-in-a-nutshell","title":"eBPF in a nutshell","text":"<p>eBPF is a technology that enables the execution of sandboxed programs in the Linux kernel. These programs can:</p> <ul> <li>Monitor and trace system calls: Allowing deep insights into application behavior.</li> <li>Filter network packets: Making it possible to capture or modify traffic dynamically.</li> <li>Collect performance metrics: Helping to identify bottlenecks or anomalies in real-time.</li> </ul> <p>Because these programs run inside the kernel, they operate with minimal overhead and at high speed, making eBPF an ideal choice for advanced observability and fault injection tasks.</p>"},{"location":"explanations/understanding-ebpf/#how-lueur-uses-ebpf-in-stealth-mode","title":"How lueur Uses eBPF in Stealth Mode","text":"<p>Traditionally, directing traffic through a proxy requires explicit configuration (e.g., setting the <code>HTTPS_PROXY</code> environment variable). lueur\u2019s stealth mode, powered by eBPF, takes a different approach:</p> <ul> <li>Transparent Traffic Capture:   lueur leverages eBPF to intercept connection attempts at the kernel level.  </li> <li>Seamless Integration:   With eBPF, there's no need to reconfigure your applications or network clients. The traffic is transparently rerouted through lueur\u2019s TCP proxy, allowing you to inject faults without modifying client behavior.</li> </ul>"},{"location":"explanations/understanding-ebpf/#benefits-for-reliability-engineering","title":"Benefits for Reliability Engineering","text":"<p>Leveraging eBPF in this way offers several advantages for engineers focused on building reliable systems:</p> <ul> <li>Zero-Configuration Overhead:   Since there's no need to explicitly set up a proxy in your applications, integrating fault injection into your workflow is simpler and less error-prone.</li> <li>Transparent Testing:   Faults are injected without any changes to the application code or environment variables. This means you can test how your application behaves under realistic conditions\u2014just as it would in production.</li> </ul>"},{"location":"explanations/understanding-ebpf/#limitations-and-future-directions","title":"Limitations and Future Directions","text":"<ul> <li>Linux-Only Support:   Currently, lueur\u2019s stealth mode using eBPF is available only on Linux. Other operating systems do not yet support eBPF, limiting this approach to Linux environments.</li> <li>Kernel Complexity:   Although eBPF programs are designed to be safe, working at the kernel level requires careful tuning and an advanced understanding of the Linux networking stack.</li> </ul>"},{"location":"explanations/understanding-ebpf/#conclusion","title":"Conclusion","text":"<p>Integrating eBPF into your reliability engineering practices with lueur opens up a new, transparent way to simulate network faults. By capturing and manipulating traffic at the kernel level, you can inject faults without altering your application\u2019s configuration\u2014providing a more realistic, production-like testing environment.</p> <p>As you embrace these advanced techniques, you\u2019ll gain deeper insights into your system\u2019s behavior under stress and be better equipped to build resilient, high-performance applications.</p>"},{"location":"explanations/what-is-lueur/","title":"What is lueur","text":"<p>sd</p>"},{"location":"explanations/why-lueur/","title":"Why lueur?","text":"<p>We are building lueur because we have met unexpected production issues which forced us to scramble, patch code at the last minute, and hope that live fixes will hold up, all under pressure.</p> <p>lueur aims to change that story. It brings reliability testing right into your daily development routine, so you\u2019re not left guessing how your code will behave under poor network conditions.</p> <p>Instead of waiting until the final line\u2014when the impacts are higher and fixes cost more\u2014lueur invites you to explore resilience as you go. It\u2019s built to help you identify weak spots early, reducing last-minute surprises and giving you more time to craft thoughtful solutions.</p> <p>What\u2019s in it for you? Hopefully, less stress. We believe more reliable systems lead to healthier operations.</p>"},{"location":"explanations/why-lueur/#features-that-work-with-you","title":"Features That Work With You","text":""},{"location":"explanations/why-lueur/#protocol-support","title":"Protocol Support","text":"<p>lueur slips into your workflow without demanding a big overhaul. Just point your traffic through its proxy and test your application as normal:</p> <ul> <li>Forward and tunnel proxy modes</li> <li>HTTP and HTTPS</li> <li>HTTP/1.1 and HTTP/2</li> <li>TCP IPv4 transparent proxy</li> <li>Scenarii automation</li> <li>eBPF stealth redirection on Linux</li> </ul>"},{"location":"explanations/why-lueur/#real-world-faults","title":"Real-World Faults","text":"<p>lueur simulates the kinds of hiccups you\u2019ve seen (or worried about) in production\u2014right on your own machine:</p> <ul> <li>Inject HTTP errors to see if your app recovers gracefully.</li> <li>Add latency and jitter to gauge performance under slow networks.</li> <li>Test packet loss and bandwidth limits to discover scaling limits.</li> <li>Introduce random \u201cbad gateway\u201d responses and ensure robust fallback paths.</li> </ul>"},{"location":"explanations/why-lueur/#tailored-for-your-needs","title":"Tailored for Your Needs","text":"<p>If your scenario demands more than the built-in faults, lueur\u2019s gRPC interface lets you customize your own conditions. Mold the tool to fit your environment, not the other way around.</p>"},{"location":"explanations/why-lueur/#lightweight-and-fast","title":"Lightweight and Fast","text":"<p>lueur wants to help you, not slow you down. It\u2019s a single binary that starts up quickly and has minimal overhead. That means you can integrate it into your daily workflow, tests, and continuous integration pipelines without feeling weighed down.</p> <p>Under the hood, lueur uses Rust to ensure speed, safety, and resilience\u2014just like what you strive for in your own code.</p>"},{"location":"explanations/why-lueur/#the-real-world-costs-of-slowness-and-unreliability","title":"The Real-World Costs of Slowness and Unreliability","text":"<p>It\u2019s not just about feeling \u201cfast.\u201d Slow or unreliable responses can have real business and user engagement costs\u2014even during early development phases. By helping you pinpoint potential performance and reliability issues early, lueur empowers you to avoid these pitfalls:</p> <ul> <li> <p>Reduced Revenue: Amazon famously found that every 100ms increase in page   load time cost them about 1% in sales (1). Sluggish endpoints aren\u2019t just an   inconvenience; they hit the bottom line.</p> </li> <li> <p>Higher Bounce Rates: According to Google, over half of mobile users abandon   a site if it takes longer than three seconds to load (2). Users today expect speed   and smoothness from the start.</p> </li> <li> <p>Decreased Engagement and Trust: Akamai\u2019s research highlights that a two-second   delay in web page load time can cause bounce rates to skyrocket (3). Slow, error-prone   services send a message of unreliability to your users\u2014something that\u2019s hard to   rebuild once trust is lost.</p> </li> </ul> <ol> <li>Greg Linden\u2019s Slides from Amazon on the cost of latency: Marissa Mayer at Web 2.0 </li> <li>Google, The Need for Mobile Speed: How Mobile Page Speed Impacts Customer Engagement (2018)  </li> <li>Akamai, Akamai Online Retail Performance Report (2017)</li> </ol> <p>What does this mean for you? By injecting faults and testing resiliency scenarios early with lueur, you\u2019re investing in a smoother launch, happier users, healthier on-calls and a product that stands strong under real-world conditions. Instead of postponing issues discovery late\u2014when they\u2019re costlier and more stressful to fix\u2014you\u2019ll tackle them when the code is fresh and flexible.</p>"},{"location":"explanations/why-lueur/#rethinking-how-we-build-software","title":"Rethinking How We Build Software","text":"<p>Traditionally, developers focus on crafting features and fixing bugs, leaving resilience concerns to be uncovered later by SREs, performance engineers, or end users in production. lueur challenges this status quo by inviting developers to think differently\u2014early, locally, and without guesswork\u2014about the resilience of their applications. This isn\u2019t just a shift in tools; it\u2019s a shift in philosophy.</p> <p>We want to help you move beyond a mindset where reliability is an afterthought. Instead, imagine it as a first-class concern in your day-to-day coding routine, as natural as running unit tests or linting your code. By experimenting with realistic fault conditions before your application ever leaves your workstation, you\u2019re not just preventing outages\u2014you\u2019re nurturing a culture of forward-thinking and robust engineering.</p>"},{"location":"explanations/why-lueur/#new-indicators-of-reliability","title":"New Indicators of Reliability","text":"<p>How can we talk about reliability in a way that resonates with developers? We propose a set of new indicators that highlight different angles of resilience:</p> <ul> <li> <p>Latency Tolerance: How gracefully does your application handle slow   network responses? Identifying how long it can wait before timing out or   degrading service helps you set meaningful SLOs (Service Level Objectives).</p> </li> <li> <p>Failure Surface Awareness: By injecting HTTP errors, packet loss, or   bandwidth constraints, you gain clarity on where your code is most fragile.   Measuring how many parts of your service break under each condition provides   a new perspective on your \u201cfailure surface.\u201d</p> </li> <li> <p>Retry Overhead: Discover the hidden costs of your application\u2019s recovery   strategies. Do you retry too aggressively, wasting resources and time?   Tracking how your code responds to fault scenarios reveals whether your   fallback paths are efficient or need fine-tuning.</p> </li> <li> <p>Resilience Debt: Like technical debt, resilience debt accumulates when you   postpone reliability fixes. Early detection and quantification of this debt   helps prioritize improvements before they become expensive production   firefights.</p> </li> </ul>"},{"location":"explanations/why-lueur/#a-daily-practice-not-a-crisis-response","title":"A Daily Practice, Not a Crisis Response","text":"<p>Think of lueur as a steady practice in your development cadence. Just as TDD (Test-Driven Development) encourages writing tests first, we envision a Reliability-First Development approach: write a feature, inject a fault, and see how it holds up. Adjust, refine, and proceed with a clearer understanding of how your software behaves under stress.</p> <p>This shift in mindset encourages you to proactively craft solutions that don\u2019t just work in ideal conditions\u2014they thrive in real-world, sometimes messy, environments. Over time, this practice becomes muscle memory, and resilience testing transforms from an occasional chore into an integral part of building software that users trust.</p> <p>lueur isn\u2019t just another tool on your belt; it\u2019s a new way of thinking about and measuring reliability. We\u2019re here to help you see beyond happy paths, to embrace uncertainty early, and to raise the bar on what \u201cdone\u201d really means.</p> <p>lueur is about making your life easier when it comes to building reliable software. It puts you in the driver\u2019s seat, letting you explore and solidify the resilience of your applications before those big, stressful moments can occur.</p>"},{"location":"how-to/install/","title":"Installing lueur","text":"<p>lueur is a designed to be easily installed on major systems such as Linux, macOS and Windows. We provide a variety of approaches to install lueur depending on your environment.</p>"},{"location":"how-to/install/#install-the-lueur-binary","title":"Install the lueur binary","text":"<p>The most direct route is to download the lueur binary on your machine.</p> <ul> <li> <p> Download lueur</p> <p>You can download the appropriate binary for your platform from here.</p> </li> <li> <p> Ensure <code>lueur</code> can be found in your <code>PATH</code></p> Linux, macOS, Windows BashWindows Powershell <pre><code>export PATH=$PATH:`pwd`\n</code></pre> <pre><code>$env:Path += ';C:\\directoy\\where\\lueur\\lives' \n</code></pre> </li> <li> <p> Turn the binary into an executable</p> <p>On Linux and macOS you will need to make sure the binary gets the executable permission flipped on with:</p> <pre><code>chmod a+x lueur\n</code></pre> </li> </ul>"},{"location":"how-to/install/#stealth-dependencies","title":"Stealth Dependencies","text":"<p>lueur stealth mode requires additional dependencies only available on Linux.</p> <ul> <li> <p> Download lueur's ebpf programs</p> <p>You can download lueur-ebpf from here.</p> </li> <li> <p> Copy them in their default location</p> <p>Move the <code>lueur-ebpf</code> binary to <code>$HOME/.local/bin</code></p> <pre><code>mv lueur-ebpf $HOME/.local/bin\n</code></pre> </li> </ul>"},{"location":"how-to/install/#install-using-cargo","title":"Install using <code>cargo</code>","text":"<p>lueur is a rust application. It can be installed using cargo which will recompile it on the machine.</p> <ul> <li> <p> Requirements</p> <p>lueur expects rust 1.85+ and the nightly channel.</p> <pre><code>rustup toolchain install nightly\n</code></pre> </li> <li> <p> Install the <code>lueur</code> executable</p> <pre><code>cargo +nightly install lueur\n</code></pre> </li> </ul>"},{"location":"how-to/install/#stealth-dependencies_1","title":"Stealth Dependencies","text":"<ul> <li> <p> Install the <code>ebpf</code> binaries on Linux</p> <p>In addition, you may also install the ebpf programs if you are running on Linux. This enables  stealth mode.</p> <pre><code>cargo +nightly install lueur-ebpf-programs --target=bpfel-unknown-none -Z build-std=core\n</code></pre> </li> </ul>"},{"location":"how-to/install/#run-as-a-container","title":"Run as a container","text":""},{"location":"how-to/install/#standard-mode","title":"Standard Mode","text":"<p>lueur is packaged as a Docker image.</p> <ul> <li> <p> Pull the Docker image </p> <pre><code>docker pull rebound/lueur:latest\n</code></pre> </li> <li> <p> Run the proxy as a container</p> <pre><code>docker run --rm -it rebound/lueur run --help\n</code></pre> </li> </ul>"},{"location":"how-to/install/#stealth-mode","title":"Stealth Mode","text":"<p>To run with stealth mode enabled, you need to pull the following image.</p> <ul> <li> <p> Pull the Docker image with ebpf enabled</p> <pre><code>docker pull rebound/lueur-ebpf:latest\n</code></pre> </li> <li> <p> Run the proxy as a container</p> <pre><code>docker run --rm -it rebound/lueur \\\n    run \\  # (1)!\n    --stealth \\  # (2)!\n    --network=host \\  # (3)!\n    --pid=host \\  # (4)!\n    --capture-process curl \\  # (5)!\n    -v /sys/fs/cgroup/:/sys/fs/cgroup/:ro  \\  # (6)!\n    --cap-add=SYS_ADMIN \\  # (7)!\n    --cap-add=BPF \\\n    --cap-add=NET_ADMIN\n</code></pre> <ol> <li>Run the lueur proxy</li> <li>Enable lueur stealth mode and loads the ebpf programs</li> <li>Attach to a particular network to view/modify its traffic. This can also be another container's network</li> <li>Attach to a particular process namespace to access its processes. This can also be another container's network</li> <li>Tell lueur to focus on traffic coming from a specific process name such as curl</li> <li>Share the host's cgroup resources with the container so the ebpf programs can be attached to them</li> <li>Give the process required capabilities for eBPF</li> </ol> </li> </ul>"},{"location":"how-to/ci/github/","title":"Execute Scenarios From GitHub Action","text":"<p>This guide will walk you through integrating lueur into your GitHub pipeline.</p>"},{"location":"how-to/ci/github/#what-youll-achieve","title":"What You'll Achieve","text":"<p>You will learn how to run a lueur scenario as part of your GitHub workflow and use the result to fail a GitHub job.</p> <p>Start your applicatiopn first</p> <p>The guides below do not show how to run the target service from within your workflow. For instance, you could run a step like this first:</p> <pre><code>  - name: Run application under test in the background\n    shell: bash\n    run: RUNNER_TRACKING_ID=\"\" &amp;&amp; (nohup ./my-app &amp;)\n</code></pre>"},{"location":"how-to/ci/github/#run-lueurs-scenario-step-by-step","title":"Run lueur's scenario - Step-by-Step","text":"<p>The basic approach to run lueur scenarios in your GitHub workflows is to use the dedicated action.</p> <ul> <li> <p> Run lueur's scenario</p> .github/workflows/reliability.yaml<pre><code>name: Run lueur scenarios\n\non:\n  workflow_dispatch:\n\njobs:\n  run-reliability-scenarios:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: rebound-how/actions/lueur@main  # (1)!\n        with:\n          scenario: scenario.yaml  # (2)!\n</code></pre> <ol> <li>Add the lueur action</li> <li>Path to a scenario file or a directory containing scenario files</li> </ol> </li> </ul>"},{"location":"how-to/ci/github/#create-an-issue-when-at-least-one-test-failed-step-by-step","title":"Create an issue when at least one test failed - Step-by-Step","text":"<ul> <li> <p> Run lueur's scenario</p> .github/workflows/reliability.yaml<pre><code>name: Run lueur scenarios\n\non:\n  workflow_dispatch:\n\njobs:\n  run-reliability-scenarios:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: rebound-how/actions/lueur@main  # (1)!\n        with:\n          scenario: scenario.yaml  # (2)!\n          report: report.md  # (3)!\n          create-issue-on-failure: \"true\"  # (4)!\n          github-token: ${{ secrets.GITHUB_TOKEN }}  # (5)!\n</code></pre> <ol> <li>Add the lueur action</li> <li>Path to a scenario file or a directory containing scenario files</li> <li>Export the report as a markdown document as it will be used as the body of the issue</li> <li>Tell the action to create the issue if at least one test failed</li> <li>Provide the github token so the operation is authenticaed appropriately. Make sure the token has write permissions</li> </ol> </li> </ul>"},{"location":"how-to/observability/traces/","title":"Enable lueur Observability","text":"<p>This guide will walk you sending traces to an Open Telemetry aware stack.</p>"},{"location":"how-to/observability/traces/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to configure lueur so it sends traces to a local Jaeger.</p>"},{"location":"how-to/observability/traces/#send-open-telemetry-traces-to-jaeger","title":"Send Open Telemetry Traces to Jaeger","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start a local Jaeger instance</p> <p>Follow the Jaeger instructions to deploy a local instance</p> </li> <li> <p> Start demo application provided by lueur</p> <pre><code>lueur demo run\n</code></pre> </li> <li> <p> Start the proxy with a basic latency fault</p> <pre><code>lueur --with-otel \\  # (1)!\n    run \\\n    --with-latency \\ \n    --latency-distribution normal \\\n    --latency-mean 300 \\\n    --latency-stddev 40\n</code></pre> <ol> <li>Configure lueur to generate and send Open Telemetry traces</li> </ol> </li> <li> <p> Send a request to the demo application routed via the proxy</p> <pre><code>curl -x http://localhost:8080 http://localhost:7070\n</code></pre> </li> <li> <p> View lueur traces</p> <p>Open your browser and view your lueur traces.</p> <p>In the following snippet, you can quickly notice the <code>~308ms</code> delay on the poll-read. </p> </li> </ul>"},{"location":"how-to/platform/run-on-kubernetes/","title":"Introduce Network Fault Into an Application Running on Kubernetes","text":"<p>This guide will walk you through emulating faults into an application running in a Kubernetes cluster.</p>"},{"location":"how-to/platform/run-on-kubernetes/#what-youll-achieve","title":"What You'll Achieve","text":"<p>You will learn how to:</p> <ul> <li>deploy lueur's proxy as a Deployment</li> <li>run lueur's scenario as a Job</li> </ul>"},{"location":"how-to/platform/run-on-kubernetes/#run-lueurs-proxy-as-a-deployment-step-by-step","title":"Run lueur's proxy as a Deployment - Step-by-Step","text":"<ul> <li> <p> Deploy lueur's demo application in the cluster</p> <p>This steps serves only the purpose of demonstrating lueur's working in a Kubernetes cluster. You can safely ignore it if you have another application you wish to try.</p> lueur-demo.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: lueur-demo\n  labels:\n    app: lueur-demo\nautomountServiceAccountToken: false\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: lueur-demo\n  labels:\n    app: lueur-demo\nspec:\n  selector:\n    app: lueur-demo\n  ports:\n    - protocol: TCP\n      port: 7070\n      targetPort: 7070\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lueur-demo\n  labels:\n    app: lueur-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lueur-demo\n  template:\n    metadata:\n      labels:\n        app: lueur-demo\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: lueur-demo\n      securityContext:\n        runAsUser: 65532\n        runAsGroup: 65532\n        fsGroup: 65532\n      containers:\n        - name: lueur-demo\n          image: rebound/lueur:latest\n          imagePullPolicy: Always\n          tty: true\n          args:\n            - demo\n            - run\n            - \"0.0.0.0\"\n            - \"7070\"\n          ports:\n            - containerPort: 7070\n              name: http\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            privileged: false\n            capabilities:\n              drop:\n                - ALL\n</code></pre> <p>Apply it as follows:</p> <pre><code>kubectl apply -f lueur-demo.yaml\n</code></pre> </li> <li> <p> Deploy lueur's proxy Kubernetes Resources</p> <p>Below is an example of running lueur's proxy as a deployment, with a single replica.</p> lueur-proxy.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: lueur-proxy\n  labels:\n    app: lueur-proxy\nautomountServiceAccountToken: false\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: lueur-proxy-config\n  labels:\n    app: lueur-proxy\ndata:\n  LUEUR_UPSTREAMS: \"http://lueur-demo:7070\" # (1)!\n  LUEUR_WITH_LATENCY: \"true\" # (2)!\n  LUEUR_LATENCY_MEAN: \"300\"\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: lueur-proxy\n  labels:\n    app: lueur-proxy\nspec:\n  selector:\n    app: lueur-proxy\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lueur-proxy\n  labels:\n    app: lueur-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lueur-proxy\n  template:\n    metadata:\n      labels:\n        app: lueur-proxy\n      annotations:\n        sidecar.istio.io/inject: \"false\"  # (3)!\n    spec:\n      serviceAccountName: lueur-proxy\n      securityContext:\n        runAsUser: 65532\n        runAsGroup: 65532\n        fsGroup: 65532\n      containers:\n        - name: lueur-proxy\n          image: rebound/lueur:latest\n          imagePullPolicy: Always\n          tty: true\n          args:\n            - --log-stdout\n            - --log-level\n            - debug\n            - run\n            - --no-ui  # (4)!\n            - --proxy-address\n            - \"0.0.0.0:8080\"  # (5)!\n          ports:\n            - containerPort: 8080\n              name: http\n          envFrom:\n            - configMapRef:\n                name: lueur-proxy-config\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            privileged: false\n            capabilities:\n              drop:\n                - ALL\n</code></pre> <ol> <li>Comma-seperated list of hosts that the proxy is allowed to impact. We resolve to the demo application via its Kubernetes service name.</li> <li>Enable a latency fault, read the reference for more details on environment variables</li> <li>Not really needed but in case you run in a Istio-aware environment, tell Istio not to add any sidecar to the proxy</li> <li>Disable the proxy terminal's UI which isn't really useful in this environment</li> <li>Make the lueur proxy address listen on a non-loopback interface to be reachable</li> </ol> <p>Apply it as follows:</p> <pre><code>kubectl apply -f lueur-proxy.yaml\n</code></pre> </li> <li> <p> Make a HTTP request to the demo service via the proxy</p> <p>First, start a throwaway curl pod. This will start a shell from it:</p> <pre><code>kubectl run lueur-test --rm -it --restart=Never --image=curlimages/curl -- sh\n</code></pre> <p>Once the pod is started and its shell available, you can run the following command from it:</p> <pre><code>curl -w \"\\nConnected IP: %{remote_ip}\\nTotal time: %{time_total}s\\n\" -x http://lueur-proxy:8080 http://lueur-demo:7070\n&lt;h1&gt;Hello, World!&lt;/h1&gt;\nConnected IP: 10.152.183.146\nTotal time: 0.315056s\n</code></pre> <p>This resolves both the proxy and the demo application from within the cluster, demonstrating a latency of roughly <code>315ms</code>.</p> <p>Once you exist the pod, its resources will be automatically released.</p> </li> </ul>"},{"location":"how-to/platform/run-on-kubernetes/#run-lueurs-scenario-as-a-job-step-by-step","title":"Run lueur's scenario as a Job - Step-by-Step","text":"<ul> <li> <p> Deploy lueur's demo application in the cluster</p> <p>This steps serves only the purpose of demonstrating lueur's working in a Kubernetes cluster. You can safely ignore it if you have another application you wish to try.</p> lueur-demo.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: lueur-demo\n  labels:\n    app: lueur-demo\nautomountServiceAccountToken: false\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: lueur-demo\n  labels:\n    app: lueur-demo\nspec:\n  selector:\n    app: lueur-demo\n  ports:\n    - protocol: TCP\n      port: 7070\n      targetPort: 7070\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lueur-demo\n  labels:\n    app: lueur-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lueur-demo\n  template:\n    metadata:\n      labels:\n        app: lueur-demo\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: lueur-demo\n      securityContext:\n        runAsUser: 65532\n        runAsGroup: 65532\n        fsGroup: 65532\n      containers:\n        - name: lueur-demo\n          image: rebound/lueur:latest\n          imagePullPolicy: Always\n          tty: true\n          args:\n            - demo\n            - run\n            - \"0.0.0.0\"\n            - \"7070\"\n          ports:\n            - containerPort: 7070\n              name: http\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            privileged: false\n            capabilities:\n              drop:\n                - ALL\n</code></pre> <p>Apply it as follows:</p> <pre><code>kubectl apply -f lueur-demo.yaml\n</code></pre> </li> <li> <p> Load a lueur scenario as a Kubernetes ConfigMap</p> <p>Let's play a simple scenario with a single test call executed 4 times in total: 12 baseline call without latency applied and three calls with latencies gradually increasing by <code>30ms</code> steps.</p> scenario.yaml<pre><code>---\ntitle: \"Latency Increase By 30ms Steps From Downstream\"\ndescription: \"\"\nscenarios:\n  - call:\n      method: GET\n      url: http://lueur-demo:7070/ping\n    context:\n      upstreams:\n        - https://postman-echo.com\n      faults:\n        - type: latency\n          mean: 80\n          stddev: 5\n          direction: ingress\n          side: client\n      strategy:\n        mode: Repeat\n        step: 30\n        count: 3\n        add_baseline_call: true\n    expect:\n      status: 200\n      response_time_under: 490\n</code></pre> <p>To load this scenario as a configmap, run the next command:</p> <pre><code>kubectl create configmap lueur-scenario-file \\\n  --from-file=scenario.yaml=scenario.yaml\n</code></pre> </li> <li> <p> Deploy lueur's scenario as a Kubernetes Job</p> <p>Below is an example of running lueur's scenarior as a job without retry.</p> lueur-scenario.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: lueur-scenario\n  labels:\n    app: lueur-scenario\nautomountServiceAccountToken: false\n\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: lueur-scenario\n  labels:\n    app: lueur-scenario\nspec:\n  backoffLimit: 0  # (1)!\n  template:\n    metadata:\n      labels:\n        app: lueur-scenario\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: lueur-scenario\n      restartPolicy: Never\n      securityContext:\n        runAsUser: 65532\n        runAsGroup: 65532\n        fsGroup: 65532\n      containers:\n        - name: lueur-scenario\n          image: rebound/lueur:latest\n          imagePullPolicy: Always\n          tty: true\n          args:\n            - scenario\n            - run\n            - --scenario\n            - rebound/scenario.yaml\n            - --result\n            - result.json    # (2)!\n            - --report\n            - report.json    # (3)!\n          volumeMounts:\n          - name: lueur-scenario-file\n            mountPath: /home/nonroot/rebound/scenario.yaml    # (4)!\n            readOnly: true\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n            privileged: false\n            capabilities:\n              drop:\n                - ALL\n      volumes:\n      - name: lueur-scenario-file\n        configMap:\n          name: lueur-scenario-file\n          items:\n            - key: scenario.yaml\n              path: scenario.yaml\n</code></pre> <ol> <li>Do not restart the job if it failed</li> <li>Results contain the detailed events of the tests and all the applied faults</li> <li>A report is a rough analysis of the results made by lueur</li> <li>Mount the scenario into the job's container</li> </ol> <p>Apply it as follows:</p> <pre><code>kubectl apply -f lueur-scenario.yaml\n</code></pre> </li> </ul>"},{"location":"how-to/platform/run-with-docker/","title":"Run lueur as a Docker Container","text":"<p>This guide will show you how can you easily introduce network faults with  Docker containers.</p>"},{"location":"how-to/platform/run-with-docker/#what-youll-achieve","title":"What You'll Achieve","text":"<p>You will learn how to use lueur in a Docker environment, either as a standalone container or as part of a set of services orchestrated with Docker Compose.</p>"},{"location":"how-to/platform/run-with-docker/#run-lueur-as-a-container-step-by-step","title":"Run lueur as A Container - Step-by-Step","text":"<ul> <li> <p> Pull the lueur image</p> <pre><code>docker pull rebound/lueur\n</code></pre> </li> <li> <p> Run lueur with a latency fault</p> <pre><code>docker run \\\n    -p 8080:8080 \\  # (1)!\n    --rm \\  # (2)!\n    -it \\  # (3)!\n    rebound/lueur \\ \n        run \\\n        --proxy-address 0.0.0.0:8080  \\ # (4)!\n        --upstream http://192.168.1.3:7070 \\  # (5)!\n        --with-latency --latency-mean 300\n</code></pre> <ol> <li>Expose the proxy port if you need to access it from the host</li> <li>Release the system resources once the container finishes</li> <li>Give the process a terminal</li> <li>The default behavior is to bind the proxy to the loopback which would prevent the proxy from being reached. Bind to all public interfaces with <code>0.0.0.0</code></li> <li>The address of the demo application we will apply the latency to</li> </ol> </li> <li> <p> Run the lueur demo using the same image</p> <pre><code>docker run \\\n    -p 7070:7070 \\  # (1)!\n    rebound/lueur \\\n        demo run 0.0.0.0  # (2)!\n</code></pre> <ol> <li>Expose the demo application port to the host</li> <li>Run the demo server and bind to all container's interfaces</li> </ol> </li> <li> <p> Make a request to the demo application and see it impacted by the proxy</p> <pre><code>curl \\\n    -w \"\\nConnected IP: %{remote_ip}\\nTotal time: %{time_total}s\\n\" \\\n    -x http://localhost:8080 \\\n    http://192.168.1.3:7070\n\n&lt;h1&gt;Hello, World!&lt;/h1&gt;\nConnected IP: ::1\nTotal time: 0.313161s\n</code></pre> </li> </ul>"},{"location":"how-to/platform/run-with-docker/#run-stealh-mode-in-a-container-step-by-step","title":"Run Stealh Mode in A Container - Step-by-Step","text":"<p>Warning</p> <p>Stealth mode gives the opportunity to intercept traffic without having to explicitely set the proxy on the client. It relies on eBPF and therefore requires a lot of provileges which likely you would not have in a production environment.</p> <ul> <li> <p> Pull the lueur image</p> <pre><code>docker pull rebound/lueur\n</code></pre> </li> <li> <p> Run lueur with a latency fault</p> <pre><code>docker run \\\n    -p 8080:8080 \\  # (1)!\n    -p 8089:8089 \\  # (2)!\n    --rm \\  # (3)!\n    -it \\  # (4)!\n    --network=host \\  # (5)!\n    --pid=host \\ # (6)!\n    -v /sys/fs/cgroup/:/sys/fs/cgroup/:ro \\ # (7)!\n    --cap-add=SYS_ADMIN \\ # (8)!\n    --cap-add=BPF \\ # (9)!\n    --cap-add=NET_ADMIN \\ # (10)!\n    rebound/lueur \\ \n        run \\\n        --stealth \\  # (11)!\n        --ebpf-proxy-port 8989 \\  # (12)!\n        --proxy-address 0.0.0.0:8080  \\  # (13)!\n        --upstream http://192.168.1.3:7070 \\  # (14)!\n        --with-latency --latency-mean 300\n</code></pre> <ol> <li>Expose the proxy port if you need to access it from the host</li> <li>Expose the eBPF proxy port</li> <li>Release the system resources once the container finishes</li> <li>Give the process a terminal</li> <li>Share the host network as in our example, the client runs on the host. You could also share another container's network instead</li> <li>Share the host porocess namespace to access the client's process</li> <li>Give access to the host's kernel resources for lueur eBPF programs to attach to</li> <li>Give too much power to the container but unfortunately we cannot reduce the scope so we need it</li> <li>Specific BPF priviledges</li> <li>lueur needs quite a bit of access to networking to do its job</li> <li>Enable stealth mode and loads eBPF programs</li> <li>By default, lueur picks up a random port for listening to traffic routed via ebBPF, but we need to set it to expose it</li> <li>The default behavior is to bind the proxy to the loopback which would prevent the proxy from being reached. Bind to all public interfaces with <code>0.0.0.0</code></li> <li>The address of the demo application we will apply the latency to. Note that this is ignored by lueur for now</li> </ol> </li> <li> <p> Run the lueur demo using the same image</p> <pre><code>docker run \\\n    -p 7070:7070 \\  # (1)!\n    rebound/lueur \\\n        demo run 0.0.0.0  # (2)!\n</code></pre> <ol> <li>Expose the demo application port to the host</li> <li>Run the demo server and bind to all container's interfaces</li> </ol> </li> <li> <p> Make a request to the demo application and see it impacted by the proxy</p> <pre><code>curl \\\n    -w \"\\nConnected IP: %{remote_ip}\\nTotal time: %{time_total}s\\n\" \\\n    http://192.168.1.3:7070\n\n&lt;h1&gt;Hello, World!&lt;/h1&gt;\nConnected IP: ::1\nTotal time: 0.313161s\n</code></pre> <p>Notice how we do not need to be explicit about routing traffic to the proxy by omitting setting <code>-x http://localhost:8080</code></p> </li> </ul>"},{"location":"how-to/proxy/faults/configure-bandwidth/","title":"Introducing Bandwidth Fault Into Your Flow","text":"<p>This guide will walk you through emulating network bandwidth degradation into your application using lueur proxy capabilities.</p>"},{"location":"how-to/proxy/faults/configure-bandwidth/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to deliberately inject network bandwidth constraints into your application flow using lueur\u2019s proxy features.</p>"},{"location":"how-to/proxy/faults/configure-bandwidth/#severe-upstream-slowdown-step-by-step","title":"Severe Upstream Slowdown - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with bandwidth set from server-side ingress</p> <pre><code>lueur \\\n    --with-bandwidth \\ # (1)!\n    --bandwidth-side server \\ # (2)!\n    --bandwidth-direction ingress \\ # (3)!\n    --bandwidth-rate 500 \\ # (4)!\n    --bandwidth-unit kbps\n</code></pre> <ol> <li>Enable the bandwidth fault support</li> <li>Apply the fault on server side</li> <li>Apply the fault on ingress</li> <li>Set a very limited bandwidth to 500kbps</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-bandwidth/#light-client-slowdown-step-by-step","title":"Light Client Slowdown - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with bandwidth set from client-side both ingress and egress</p> <pre><code>lueur \\\n    --with-bandwidth \\ # (1)!\n    --bandwidth-side client \\ # (2)!\n    --bandwidth-direction both \\ # (3)!\n    --bandwidth-rate 1 \\ # (4)!\n    --bandwidth-unit mbps\n</code></pre> <ol> <li>Enable the bandwidth fault support</li> <li>Apply the fault on client side</li> <li>Apply the fault on ingress and egress</li> <li>Set a reduced bandwidth to 1mbps</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-bandwidth/#throughput-degradation-step-by-step","title":"Throughput Degradation - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with bandwidth set from server-side both ingress and egress</p> <pre><code>lueur \\\n    --with-bandwidth \\ # (1)!\n    --bandwidth-side server \\ # (2)!\n    --bandwidth-direction both \\ # (3)!\n    --bandwidth-rate 2 \\ # (4)!\n    --bandwidth-unit mbps\n</code></pre> <ol> <li>Enable the bandwidth fault support</li> <li>Apply the fault on server side</li> <li>Apply the fault on ingress and egress</li> <li>Set a reduced bandwidth to 2mbps</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-http-error/","title":"Introducing HTTP Fault Into Your Flow","text":"<p>This guide will walk you through emulating application level HTTP errors into your application using lueur proxy capabilities.</p>"},{"location":"how-to/proxy/faults/configure-http-error/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to deliberately inject HTTP errors into your application flow using lueur\u2019s proxy features.</p>"},{"location":"how-to/proxy/faults/configure-http-error/#constant-internal-server-error-step-by-step","title":"Constant Internal Server Error - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with HTTP Error 500 from the remote server</p> <pre><code>lueur \\\n    --with-http-response \\ # (1)!\n    --http-status 500 \\ # (2)!\n    --http-response-trigger-probability 1  # (3)!\n</code></pre> <ol> <li>Enable the HTTP error fault support</li> <li>Set the status to 500</li> <li>Set the error on all responses</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-http-error/#intermittent-service-unavailable-errors-step-by-step","title":"Intermittent Service Unavailable Errors - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with HTTP Error 503 from the remote server</p> <pre><code>lueur \\\n    --with-http-response \\ # (1)!\n    --http-status 503 \\ # (2)!\n    --http-response-trigger-probability 0.5  # (3)!\n</code></pre> <ol> <li>Enable the HTTP error fault support</li> <li>Set the status to 503</li> <li>Set the error on half of the responses</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-http-error/#intermittent-not-found-errors-step-by-step","title":"Intermittent Not Found Errors - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with HTTP Error 404 from the remote server</p> <pre><code>lueur \\\n    --with-http-response \\ # (1)!\n    --http-status 404 \\ # (2)!\n    --http-response-trigger-probability 0.5 \\ # (3)!\n    --http-body '{\"error\": \"true\"}' # (4)!\n</code></pre> <ol> <li>Enable the HTTP error fault support</li> <li>Set the status to 404</li> <li>Set the error on half of the responses</li> <li>Set a JSON response body</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-jitter/","title":"Introducing Jitter Fault Into Your Flow","text":"<p>This guide will walk you through emulating network jitter into your application using lueur proxy capabilities.</p>"},{"location":"how-to/proxy/faults/configure-jitter/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to deliberately inject network jittering into your application flow using lueur\u2019s proxy features.</p>"},{"location":"how-to/proxy/faults/configure-jitter/#light-ingress-jitter-step-by-step","title":"Light Ingress Jitter - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with jitter on ingress</p> <pre><code>lueur \\\n    --with-jitter \\ # (1)!\n    --jitter-amplitude 30 \\ # (2)!\n    --jitter-frequency 5 \\ # (3)!\n    --jitter-direction ingress # (4)!\n</code></pre> <ol> <li>Enable the jitter fault support</li> <li>Set the amplitude (maximum delay applied to packets)</li> <li>Set the frequency (how often jitter is applied per second)</li> <li>Apply the fault on ingress</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-jitter/#strong-egress-jitter-step-by-step","title":"Strong Egress Jitter - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with jitter on egress</p> <pre><code>lueur \\\n    --with-jitter \\ # (1)!\n    --jitter-amplitude 50 \\ # (2)!\n    --jitter-frequency 10 \\ # (3)!\n    --jitter-direction egress # (4)!\n</code></pre> <ol> <li>Enable the jitter fault support</li> <li>Set the amplitude (maximum delay applied to packets)</li> <li>Set the frequency (how often jitter is applied per second)</li> <li>Apply the fault on egress</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-jitter/#bidirectional-jitter-step-by-step","title":"Bidirectional Jitter - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with jitter on egress and ingress</p> <pre><code>lueur \\\n    --with-jitter \\ # (1)!\n    --jitter-amplitude 30 \\ # (2)!\n    --jitter-frequency 8 \\ # (3)!\n    --jitter-direction both # (4)!\n</code></pre> <ol> <li>Enable the jitter fault support</li> <li>Set the amplitude (maximum delay applied to packets)</li> <li>Set the frequency (how often jitter is applied per second)</li> <li>Apply the fault on egress and ingress</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/","title":"Introducing Latency Fault Into Your Flow","text":"<p>This guide will walk you through introducing network latency into your application using lueur proxy capabilities.</p>"},{"location":"how-to/proxy/faults/configure-latency/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to deliberately inject network latency into your application flow using lueur\u2019s proxy features. By exploring different latency distributions, you'll gain insights into how your system behaves under varying network conditions.</p>"},{"location":"how-to/proxy/faults/configure-latency/#normal-distribution-step-by-step","title":"Normal Distribution - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with a normal distribution latency</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-distribution normal \\ # (2)!\n    --latency-mean 300 \\ # (3)!\n    --latency-stddev 40 # (4)!\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Use the normal distribution</li> <li>Introduce a latency of 300ms on average</li> <li>Add 40ms of jitter</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#uniform-distribution-step-by-step","title":"Uniform Distribution - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with a uniform distribution latency</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-distribution uniform \\ # (2)!\n    --latency-min 300 \\ # (3)!\n    --latency-max 500 # (4)!\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Use the uniform distribution</li> <li>Introduce a latency of at least 300ms</li> <li>Set the maximum latency to 500ms</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#pareto-distribution-step-by-step","title":"Pareto Distribution - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with a Pareto distribution latency</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-distribution pareto \\ # (2)!\n    --latency-scale 20 \\ # (3)!\n    --latency-shape 1.5 # (4)!\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Use the pareto distribution</li> <li>Set a scale of 20ms</li> <li>Set the shape of the distribution to 1.5</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#pareto-normal-distribution-step-by-step","title":"Pareto Normal Distribution - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with a Pareto distribution latency</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-distribution pareto \\ # (2)!\n    --latency-scale 20 \\ # (3)!\n    --latency-shape 1.5 \\ # (4)!\n    --latency-mean 50 \\ # (5)!\n    --latency-stddev 15 # (6)!\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Use the pareto distribution</li> <li>Set a scale of 20ms</li> <li>Set the shape of the distribution to 1.5</li> <li>Set a mean of 50ms on average</li> <li>Add 15ms of jitter</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#latency-on-ingress-only-step-by-step","title":"Latency On Ingress Only - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with any distribution and set the direction to ingress.</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-direction ingress \\ # (2)!\n    --latency-mean 50\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Set the latency to take place in ingress</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#latency-on-egress-only-step-by-step","title":"Latency On Egress Only - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with any distribution and set the direction to egress.</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-direction egress \\ # (2)!\n    --latency-mean 50\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Set the latency to take place in egress</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#latency-on-client-side-only-step-by-step","title":"Latency On Client-Side Only - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with any distribution and set the side to client.</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-side client \\ # (2)!\n    --latency-mean 50\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Set the latency to take place on client side</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#latency-on-server-side-only-step-by-step","title":"Latency On Server-Side Only - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with any distribution and set the side to server.</p> <pre><code>lueur run \\\n    --with-latency \\ # (1)!\n    --latency-side server \\ # (2)!\n    --latency-mean 50\n</code></pre> <ol> <li>Enable the latency fault support</li> <li>Set the latency to take place on server side</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-latency/#latency-on-ingress-from-server-side-only-step-by-step","title":"Latency On Ingress From Server-Side Only - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with any distribution and set the direction to ingress and the side to server.</p> <pre><code>lueur run \\\n    --with-latency \\\n    --latency-direction ingress \\\n    --latency-side server \\\n    --latency-mean 50\n</code></pre> </li> </ul>"},{"location":"how-to/proxy/faults/configure-packet-loss/","title":"Introducing Packet Loss Fault Into Your Flow","text":"<p>This guide will walk you through emulating network packet loss into your application using lueur proxy capabilities.</p>"},{"location":"how-to/proxy/faults/configure-packet-loss/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to deliberately inject network packet loss issues into your application flow using lueur\u2019s proxy features.</p>"},{"location":"how-to/proxy/faults/configure-packet-loss/#client-packet-loss-step-by-step","title":"Client Packet Loss - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with packet loss from client side</p> <pre><code>lueur \\\n    --with-packet-loss \\ # (1)!\n    --packet-loss-side client # (2)!\n</code></pre> <ol> <li>Enable the packet-loss fault support</li> <li>Apply the fault on client side</li> </ol> </li> </ul>"},{"location":"how-to/proxy/faults/configure-packet-loss/#server-packet-loss-step-by-step","title":"Server Packet Loss - Step-by-Step","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Start the proxy with packet loss from server side</p> <pre><code>lueur \\\n    --with-packet-loss \\ # (1)!\n    --packet-loss-side server # (2)!\n</code></pre> <ol> <li>Enable the packet-loss fault support</li> <li>Apply the fault on server side</li> </ol> </li> </ul>"},{"location":"how-to/proxy/stealth/configure-stealth-mode/","title":"Intercept Network Traffic Transparently","text":"<p>This guide will walk you through enabling lueur's stealth mode to capture network traffic without modifying your application.</p> <p>Info</p> <p>This feature is only available on Linux as it relies on a kernel advanaced capability called ebpf.</p>"},{"location":"how-to/proxy/stealth/configure-stealth-mode/#what-youll-achieve","title":"What You'll Achieve","text":"<p>In this guide, you\u2019ll learn how to intercept network traffic coming from a particular process on your machine without having to configure it to explicitely re-route via the proxy.</p>"},{"location":"how-to/proxy/stealth/configure-stealth-mode/#capture-https-traffic","title":"Capture HTTPS Traffic","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Install lueur's ebpf dependencies</p> <p>Follow the procedure to install the eBPF programs on your machine.</p> </li> <li> <p> Start the proxy in stealth mode with a normal distribution latency</p> <pre><code>lueur run \\\n    --stealth \\ # (1)!\n    --capture-process curl \\ # (2)!\n    --with-latency \\ # (3)!\n    --latency-mean 300 \\\n    --latency-stddev 40\n</code></pre> <ol> <li>Enable stealth mode</li> <li>Stealth mode will focus only on processes named <code>curl</code></li> <li>Enable the latency fault support</li> </ol> </li> <li> <p> Send traffic</p> <pre><code>curl \\\n    -4 \\ # (1)!\n    -I \\ # (2)!\n    -o /dev/null -s \\ # (3)!\n    -w \"Connected IP: %{remote_ip}\\nTotal time: %{time_total}s\\n\" \\ # (4)!\n    https://www.google.com\n</code></pre> <ol> <li>lueur can only intercept IPv4 traffic</li> <li>Let's only focus on a HEAD request for brevety</li> <li>Discard any returned output</li> <li>Display statistics about the call</li> </ol> </li> </ul>"},{"location":"how-to/proxy/stealth/configure-stealth-mode/#apply-latency-to-a-postgresql-connection","title":"Apply Latency to a PostgreSQL Connection","text":"<ul> <li> <p> Install lueur</p> <p>Follow the procedure to install lueur on your machine.</p> </li> <li> <p> Install lueur's ebpf dependencies</p> <p>Follow the procedure to install the eBPF programs on your machine.</p> </li> <li> <p> Start a local PostgreSQL server using a container</p> <pre><code>docker run \\\n    --name demo-db \\ # (1)!\n    -e POSTGRES_USER=demo \\ # (2)!\n    -e POSTGRES_PASSWORD=demo \\ # (3)!\n    -e POSTGRES_DB=demo \\ # (4)!\n    --rm -it \\ # (5)!\n    -p 5432:5432 \\ # (6)!\n    postgres\n</code></pre> <ol> <li>Name of the container, useful to identify and delete it later on</li> <li>Default basic user named {demo}</li> <li>Password set to {demo} for the user {demo}</li> <li>Default database name</li> <li>Release all resources once we stop the container</li> <li>Expose the database port onto the host</li> </ol> </li> <li> <p> Start the proxy in stealth mode with a normal distribution latency</p> <pre><code>lueur run \\\n    --stealth \\ # (1)!\n    --capture-process curl \\ # (2)!\n    --with-latency \\ # (3)!\n    --latency-mean 300 \\\n    --latency-stddev 40\n</code></pre> <ol> <li>Enable stealth mode</li> <li>Stealth mode will focus only on processes named <code>curl</code></li> <li>Enable the latency fault support</li> </ol> </li> <li> <p> Communicate with your PostgreSQL server</p> <p>First, install <code>uv</code> to run the demonstration script below. Follow the instructions from the uv documentation.</p> <p>Let's use the following basic Python script:</p> connect-to-pgsql.py<pre><code>import time\n\nimport psycopg\n\n\ndef query_database_server_time(url: str) -&gt; None:\n    start = time.time()\n\n    with psycopg.Connection.connect(url) as conn: # (1)!\n        cur = conn.execute(\"select now()\")\n        print(cur.fetchone()[0])\n\n    print(f\"Time taken {time.time() - start}\")\n\n\nif __name__ == \"__main__\":\n    connection_url = \"postgresql://demo:demo@localhost:5432/demo\" # (2)!\n\n    query_database_server_time(connection_url)\n</code></pre> <ol> <li>We are using a context manager which closes the connection automatically</li> <li>This should reflect the address of your PostgreSQL database</li> </ol> <p>Run the script using <code>uv</code>.</p> <pre><code>uv run \\ # (1)!\n    --with psycopg[binary] \\  # (2)!\n    python connect-to-pgsql.py\n</code></pre> <ol> <li>Use uv to run the script with the required dependency</li> <li>Install the required dependency on the fly. Here the psycopg driver</li> </ol> <p>This should output something such as:</p> <pre><code>2025-03-08 13:06:16.968350+00:00\nTime taken 0.30957818031311035  # (1)!\n</code></pre> <ol> <li>This shows the impact of the latency injected by lueur into the exchange</li> </ol> <p>Info</p> <p>We use <code>uv</code> to ease the management of the Python environment for this particular script. When we run the script this way, the actual process executing the script is indeed <code>python</code>. This is why lueur captures the network traffic from the <code>python</code> process, not from <code>uv</code>.</p> </li> </ul>"},{"location":"reference/cli-commands/","title":"CLI Reference","text":"<p>This document provides an overview of the CLI. The CLI is organized into a single command with grouped parameters, allowing you to configure and run the proxy with various network fault simulations, execute test scenarios defined in a file or launch a local demo server.</p>"},{"location":"reference/cli-commands/#commands","title":"Commands","text":""},{"location":"reference/cli-commands/#run","title":"<code>run</code>","text":"<p>Run the proxy with fault injection enabled. This command applies the specified network faults to HTTP requests and tunnel streams.</p>"},{"location":"reference/cli-commands/#scenario","title":"<code>scenario</code>","text":"<p>Execute a predefined fault injection scenario. This command includes additional subcommands for selecting different scenarios.</p>"},{"location":"reference/cli-commands/#demo","title":"<code>demo</code>","text":"<p>Run a simple demo server for learning purposes, with various fault simulation options available.</p>"},{"location":"reference/cli-commands/#global-options","title":"Global Options","text":"<p>These options apply across all commands:</p> <ul> <li> <p><code>--log-file &lt;file&gt;</code> Path to a file where lueur can append new logs during its execution. Example: <code>--log-file lueur.log</code></p> </li> <li> <p><code>--log-stdout</code> Flag enabling logs to be printed to the standard output. Default: Disabled Example: <code>--log-stdout</code></p> </li> <li> <p><code>--log-level &lt;level&gt;</code> Logging level which must follow the format set by cargo. Default: <code>info,tower_http=debug</code> Example: <code>--log-level warning</code></p> </li> </ul>"},{"location":"reference/cli-commands/#observability-options","title":"Observability Options","text":"<p>These options apply across all commands:</p> <ul> <li><code>--with-otel</code> Enable Open Telemetry traces and metrics. Expects the correct Open Telemetry environment variables to be configured. Default: Disabled Example: <code>--with-otel</code></li> </ul>"},{"location":"reference/cli-commands/#run-command-options","title":"<code>run</code> Command Options","text":"<p>Fault injection parameters are grouped into sections based on the type of network fault. Each section allows you to enable or disable a fault and configure its properties.</p>"},{"location":"reference/cli-commands/#proxy-configuration-options","title":"Proxy Configuration Options","text":"<p>These options define how to comnfigure the proxy started by lueur:</p> <ul> <li> <p><code>--proxy-address &lt;address&gt;</code> Listening address for the proxy server. Default: <code>127.0.0.1:8080</code> Example: <code>--proxy-address 192.168.12.45:8090</code></p> </li> <li> <p><code>--upstream &lt;host&gt;</code> Target host(s) to proxy (can be specified multiple times). Example: <code>--upstream example.com</code></p> </li> <li> <p><code>--grpc-plugin &lt;address&gt;</code> Specify one or more gRPC plugin addresses. Example: <code>--grpc-plugin 192.168.1.100:50051</code></p> </li> </ul>"},{"location":"reference/cli-commands/#stealth-configuration-options","title":"Stealth Configuration Options","text":"<p>These options configure the stealth mode of the lueur's proxy.</p> <p>Info</p> <p>Stealth mode is currently only supported on Linux hosts. Therefore you will not see these options on other systems.</p> <p>This option addresses recent Linux kernels.</p> <p>Note</p> <p>Upstream hosts are currently ignored when traffic is redirected via the eBPF programs.</p> <ul> <li> <p><code>--stealth</code> Enable stealth mode using eBPF. Default: Disabled Example: <code>--stealth</code></p> </li> <li> <p><code>--capture-process &lt;procname&gt;</code> Process name captured by the eBPF program (requires <code>--stealth</code>). Example: <code>--capture-process curl</code></p> </li> <li> <p><code>--ebpf-programs-dir &lt;directory&gt;</code> Directory path where lueur eBPF programs are located (requires <code>--stealth</code>). Default: <code>$HOME/.local/bin</code> Example: <code>--ebpf-programs-dir /tmp/somewhere</code></p> </li> </ul>"},{"location":"reference/cli-commands/#latency-options","title":"Latency Options","text":"<ul> <li> <p><code>--with-latency</code>   Enable latency fault injection. Default: Disabled</p> </li> <li> <p><code>--latency-per-read-write</code>   Apply latency on each read or write operation rather than once. Default: Disabled</p> </li> <li> <p><code>--latency-side &lt;side&gt;</code>   Side to apply the latency fault. Options: <code>client</code>, <code>server</code> Default: <code>server</code></p> </li> <li> <p><code>--latency-direction &lt;direction&gt;</code>   Direction to apply the latency fault. Options: <code>ingress</code>, <code>egress</code>, <code>both</code> Default: <code>both</code></p> </li> <li> <p><code>--latency-distribution &lt;distribution&gt;</code>   Latency distribution type (uniform, normal, pareto, pareto_normal). Default: <code>normal</code></p> </li> <li> <p><code>--latency-mean &lt;value&gt;</code>   Mean latency in milliseconds (positive float) (requires <code>--latency-distribution normal</code>). Example: <code>--latency-mean 300</code></p> </li> <li> <p><code>--latency-stddev &lt;value&gt;</code>   Standard deviation in milliseconds (non-negative float) (requires <code>--latency-distribution normal</code>). Example: <code>--latency-stddev 20</code></p> </li> <li> <p><code>--latency-shape &lt;value&gt;</code>   Distribution shape parameter (non-negative float) (requires <code>--latency-distribution pareto|pareto_normal</code>). Example: <code>--latency-shape 20</code></p> </li> <li> <p><code>--latency-scale &lt;value&gt;</code>   Distribution scale parameter (non-negative float) (requires <code>--latency-distribution pareto|pareto_normal</code>). Example: <code>--latency-scale 20</code></p> </li> <li> <p><code>--latency-min &lt;value&gt;</code>   Minimum latency for uniform distribution (non-negative float) (requires <code>--latency-distribution uniform</code>). Example: <code>--latency-min 20</code></p> </li> <li> <p><code>--latency-max &lt;value&gt;</code>   Maximum latency for uniform distribution (non-negative float) (requires <code>--latency-distribution uniform</code>). Example: <code>--latency-max 50</code></p> </li> </ul>"},{"location":"reference/cli-commands/#bandwidth-options","title":"Bandwidth Options","text":"<ul> <li> <p><code>--with-bandwidth</code>   Enable bandwidth fault injection. Default: Disabled</p> </li> <li> <p><code>--bandwidth-side &lt;side&gt;</code>   Side to apply the bandwidth fault. Options: <code>client</code>, <code>server</code> Default: <code>server</code></p> </li> <li> <p><code>--bandwidth-direction &lt;direction&gt;</code>   Direction to apply the bandwidth fault. Options: <code>ingress</code>, <code>egress</code>, <code>both</code> Default: <code>ingress</code></p> </li> <li> <p><code>--bandwidth-rate &lt;value&gt;</code>   Bandwidth rate as a positive integer. Default: <code>1000</code></p> </li> <li> <p><code>--bandwidth-unit &lt;unit&gt;</code>   Unit for the bandwidth rate (options: Bps, KBps, MBps, GBps). Default: <code>Bps</code></p> </li> </ul>"},{"location":"reference/cli-commands/#jitter-options","title":"Jitter Options","text":"<ul> <li> <p><code>--with-jitter</code>   Enable jitter fault injection. Default: Disabled</p> </li> <li> <p><code>--jitter-direction &lt;direction&gt;</code>   Direction to apply the jitter fault. Options: <code>ingress</code>, <code>egress</code>, <code>both</code> Default: <code>ingress</code></p> </li> <li> <p><code>--jitter-amplitude &lt;value&gt;</code>   Maximum jitter delay in milliseconds (non-negative float). Default: <code>20.0</code></p> </li> <li> <p><code>--jitter-frequency &lt;value&gt;</code>   Frequency of jitter application in Hertz times per second (non-negative float). Default: <code>5.0</code></p> </li> </ul>"},{"location":"reference/cli-commands/#dns-options","title":"DNS Options","text":"<ul> <li> <p><code>--with-dns</code>   Enable DNS fault injection. Default: Disabled</p> </li> <li> <p><code>--dns-rate &lt;value&gt;</code>   Probability to trigger a DNS failure (non-negative float). Default: <code>0.5</code></p> </li> </ul>"},{"location":"reference/cli-commands/#packet-loss-options","title":"Packet Loss Options","text":"<ul> <li> <p><code>--with-packet-loss</code>   Enable packet loss fault injection. Default: Disabled</p> </li> <li> <p><code>--packet-loss-direction &lt;direction&gt;</code>   Direction to apply the packet loss fault. Options: <code>ingress</code>, <code>egress</code>, <code>both</code> Default: <code>ingress</code></p> </li> </ul>"},{"location":"reference/cli-commands/#http-response-options","title":"HTTP Response Options","text":"<ul> <li> <p><code>--with-http-response</code>   Enable HTTP response fault injection (return a predefined response). Default: Disabled</p> </li> <li> <p><code>--http-response-direction &lt;direction&gt;</code>   Direction to apply the HTTP response fault. Options: <code>ingress</code>, <code>egress</code>, <code>both</code> Default: <code>ingress</code></p> </li> <li> <p><code>--http-status &lt;code&gt;</code>   HTTP status code to return (e.g., 500, 503). Default: <code>500</code></p> </li> <li> <p><code>--http-body &lt;string&gt;</code>   Optional response body to return. Default: (none)</p> </li> <li> <p><code>--http-response-trigger-probability &lt;value&gt;</code>   Probability (0.0 to 1.0) to trigger the HTTP response fault. Default: <code>1.0</code> (always trigger when enabled)</p> </li> </ul>"},{"location":"reference/cli-commands/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/cli-commands/#running-the-proxy-with-multiple-faults","title":"Running the Proxy with Multiple Faults","text":"<pre><code>lueur run \\\n  --proxy-address \"127.0.0.1:8080\" \\\n  --with-latency --latency-mean 120.0 --latency-stddev 30.0 \\\n  --with-bandwidth --bandwidth-rate 2000 --bandwidth-unit KBps\n</code></pre>"},{"location":"reference/cli-commands/#scenario-command-options","title":"<code>scenario</code> Command Options","text":"<p>A lueur scenario is a file containing test scenarios to execute automatically by lueur generating report and result files for further analysis.</p>"},{"location":"reference/cli-commands/#proxy-configuration-options_1","title":"Proxy Configuration Options","text":"<ul> <li><code>--proxy-address &lt;address&gt;</code> Listening address for the proxy server. Default: <code>127.0.0.1:8080</code> Example: <code>--proxy-address 192.168.12.45:8090</code></li> </ul>"},{"location":"reference/cli-commands/#scenario-options","title":"Scenario Options","text":"<ul> <li> <p><code>--scenario &lt;file&gt;</code> Path to a YAML scenario file. Example: <code>--scenario ./scenario.yaml</code></p> </li> <li> <p><code>--report &lt;file&gt;</code> Path to a file where to save the final repor. Example: <code>--scenario ./report.yaml</code></p> </li> </ul>"},{"location":"reference/cli-commands/#demo-command-options","title":"<code>demo</code> Command Options","text":"<p>A simple demo server listening for HTTP requests.</p>"},{"location":"reference/cli-commands/#demo-options","title":"Demo Options","text":"<ul> <li> <p><code>--address &lt;addr&gt;</code> IP address to bind the the demo server to. Default: <code>127.0.0.1</code> Example: <code>--address 192.168.2.34</code></p> </li> <li> <p><code>--port &lt;port&gt;</code> Port to bind to. Default: <code>7070</code> Example: <code>--port 8989</code></p> </li> </ul>"},{"location":"reference/environment-variables/","title":"Environment Variables","text":"<p>lueur is configured through its CLI arguments. However, in some cases, it may be simpler to populate these options via environment variables.</p>"},{"location":"reference/environment-variables/#common-variables","title":"Common Variables","text":"Name Default Value Explanation <code>LUEUR_LOG_FILE</code> (none) Path to a file where to write lueur logs <code>LUEUR_WITH_STDOUT_LOGGING</code> <code>false</code> Whether to enable logging to stdout <code>LUEUR_LOG_LEVEL</code> <code>info,tower_http=debug</code> Level respecting tracing subscriber env filter directives"},{"location":"reference/environment-variables/#observability-variables","title":"Observability Variables","text":"Name Default Value Explanation <code>LUEUR_WITH_OTEL</code> <code>false</code> Whether to enable Open Telemetry tracing and metrics"},{"location":"reference/environment-variables/#run-command-variables","title":"<code>run</code> Command Variables","text":"Name Default Value Explanation <code>LUEUR_PROXY_ADDRESS</code> <code>127.0.0.1:8080</code> The address on which the proxy server listens. <code>LUEUR_ENABLE_STEALTH</code> <code>false</code> Whether stealth mode (using eBPF) is enabled. <code>LUEUR_EBPF_PROCESS_NAME</code> (none) The name of a process to intercept traffic from (used when stealth mode is enabled). <code>LUEUR_EBPF_PROGRAMS_DIR</code> <code>\"$HOME/cargo/bin\"</code> The directory where eBPF programs for lueur can be found (used when stealth mode is enabled). <code>LUEUR_GRPC_PLUGINS</code> (none) Comma-separated list of gRPC plugin addresses. <code>LUEUR_UPSTREAMS</code> (none) Comma-separated list of upstream hostnames to proxy. <code>LUEUR_WITH_LATENCY</code> <code>false</code> Whether a latency fault is enabled. <code>LUEUR_LATENCY_PER_READ_WRITE</code> <code>false</code> Whether latency should be applied on a per read/write operation or once. <code>LUEUR_LATENCY_DISTRIBUTION</code> <code>normal</code> The statistical distribution used. <code>LUEUR_LATENCY_SIDE</code> <code>server</code> The side which will be impacted by the fault. <code>LUEUR_LATENCY_DIRECTION</code> <code>ingress</code> The direction which will be impacted by the fault. <code>LUEUR_LATENCY_MEAN</code> (none) Mean latency in milliseconds for latency fault injection. <code>LUEUR_LATENCY_STANDARD_DEVIATION</code> (none) Standard deviation of latency in milliseconds. <code>LUEUR_LATENCY_SHAPE</code> (none) Distribution shape when using pareto or pareto normal. <code>LUEUR_LATENCY_SCALE</code> (none) Distribution scale when using pareto or pareto normal. <code>LUEUR_LATENCY_MIN</code> (none) Minimum latency when using a uniform distribution. <code>LUEUR_LATENCY_MAX</code> (none) Maximum latency when using a uniform distribution. <code>LUEUR_WITH_BANDWIDTH</code> <code>false</code> Whether a bandwidth fault is enabled. <code>LUEUR_BANDWIDTH_DIRECTION</code> <code>ingress</code> The direction which will be impacted by the fault. <code>LUEUR_BANDWIDTH_RATE</code> <code>1000</code> Rate to impose on traffic. <code>LUEUR_BANDWIDTH_UNIT</code> <code>bps</code> Unit of the rate. <code>LUEUR_WITH_JITTER</code> <code>false</code> Whether a jitter fault is enabled. <code>LUEUR_JITTER_AMPLITUDE</code> <code>20.0</code> Maximum jitter delay in milliseconds for jitter fault injection. <code>LUEUR_JITTER_FREQ</code> <code>5.0</code> Frequency (in Hertz) of jitter application. <code>LUEUR_WITH_PACKET_LOSS</code> <code>false</code> Whether a packet-loss fault is enabled. <code>LUEUR_PACKET_LOSS_SIDE</code> <code>server</code> The side which will be impacted by the fault. <code>LUEUR_PACKET_LOSS_DIRECTION</code> <code>ingress</code> The direction which will be impacted by the fault. <code>LUEUR_WITH_HTTP_FAULT</code> <code>false</code> Whether a http fault fault is enabled. <code>LUEUR_HTTP_FAULT_STATUS</code> <code>500</code> HTTP status code to return when the HTTP response fault is triggered. <code>LUEUR_HTTP_FAULT_PROBABILITY</code> <code>1.0</code> Probability to apply the fault on a given HTTP exchange. <code>LUEUR_WITH_DNS</code> <code>false</code> Whether a dns fault is enabled. <code>LUEUR_DNS_PROBABILITY</code> <code>0.5</code> Probability (0\u2013100) to trigger a DNS fault."},{"location":"reference/environment-variables/#scenario-command-variables","title":"<code>scenario</code> Command Variables","text":"Name Default Value Explanation <code>LUEUR_SCENARIO_REPORT_PATH</code> (none) The file path to a scenario file or a directory path to a folder containing scenario files. <code>LUEUR_SCENARIO_PROXY_ADDR</code> <code>127.0.0.1:8080</code> Address of the proxy the secanrio command will run during the tests"},{"location":"reference/environment-variables/#demo-command-variables","title":"<code>demo</code> Command Variables","text":"Name Default Value Explanation <code>LUEUR_DEMO_ADDR</code> <code>127.0.0.1</code> IP address to bind the server to. <code>LUEUR_DEMO_PORT</code> <code>7070</code> Port to bind the server to."},{"location":"reference/scenario-file-format/","title":"Scenario File Format","text":""},{"location":"reference/scenario-file-format/#scenario-overview","title":"Scenario Overview","text":"<p>A Lueur scenario file is a structured document that defines a suite of tests designed to simulate adverse network conditions and assess your application's resilience. Rather than being an arbitrary collection of tests, each scenario file follows a consistent structure that ensures clarity, repeatability, and ease of automation.</p> <p>At the top level, a scenario file contains metadata\u2014such as a title and an optional description\u2014that provides context for the entire test suite. This is followed by a collection of individual test cases, each of which is known as a scenario item.</p> <p>Each scenario item is composed of three primary components:</p> <p>Call: This section describes the HTTP request that will be executed during the test. It specifies essential details such as the HTTP method (for example, GET or POST), the target URL, and any headers or body content that are required. Essentially, it outlines the action that triggers the fault injection.</p> <p>Context: The context defines the environment in which the test runs. It lists the upstream endpoints that will be affected by fault injection and specifies the type of faults to simulate. Faults can include network latency, packet loss, bandwidth restrictions, jitter, DNS anomalies, or HTTP errors. Additionally, an optional strategy can be included to repeat or vary the test conditions systematically\u2014allowing you to incrementally adjust parameters such as latency.</p> <p>Expectation: This component sets the criteria for a successful test. It defines what outcomes are acceptable by specifying expected HTTP status codes and performance metrics like maximum response times. By clearly stating these expectations, the scenario file provides a benchmark against which the test results can be measured.</p> <p>The structured approach of a scenario file not only helps maintain consistency across tests but also simplifies troubleshooting and iterative refinement. For detailed information on individual fault parameters\u2014such as the mean and standard deviation for latency or the rate limits for bandwidth\u2014refer to the relevant definitions. This ensures that each test case is both precise and aligned with your reliability objectives.</p>"},{"location":"reference/scenario-file-format/#example","title":"Example","text":"<p>The following example demonstrates a scenario file with many tests and their expectations. It targets the lueur demo application.</p> scenario.yaml<pre><code>---\ntitle: \"Latency Increase By 30ms Steps From Downstream\"\ndescription: \"\"\nscenarios:\n  - call:\n      method: GET\n      url: http://localhost:7070/ping\n    context:\n      upstreams:\n        - https://postman-echo.com\n      faults:\n        - type: latency\n          mean: 80\n          stddev: 5\n          direction: ingress\n          side: client\n      strategy:\n        mode: Repeat\n        step: 30\n        count: 3\n        add_baseline_call: true\n    expect:\n      status: 200\n      response_time_under: 490\n\n---\ntitle: \"Within Allowed Latency While Bandwidth At 5 bytes/second\"\ndescription: \"\"\nscenarios:\n  - call:\n      method: POST\n      url: http://localhost:7070/uppercase\n      headers:\n        \"Content-Type\": \"application/json\"\n      body: '{\"content\": \"hello\"}'\n    context:\n      upstreams:\n        - http://localhost:7070\n      faults:\n        - type: bandwidth\n          rate: 5\n          unit: Bps\n          direction: ingress\n    expect:\n      response_time_under: 8\n\n---\ntitle: \"Circuit Breaker Takes Care of 404\"\ndescription: \"\"\nscenarios:\n  - call:\n      method: GET\n      url: http://localhost:7070/ping/myself\n    context:\n      upstreams:\n        - http://127.0.0.1:7070\n      faults:\n        - type: httperror\n          status_code: 404\n          probability: 0.9\n    expect:\n      status: 200\n\n---\ntitle: \"Packet loss has no impact on service performance\"\ndescription: \"\"\nscenarios:\n  - call:\n      method: GET\n      url: http://localhost:7070/ping\n    context:\n      upstreams:\n        - https://postman-echo.com\n      faults:\n        - type: packetloss\n          direction: ingress\n          side: client\n    expect:\n      status: 200\n</code></pre> <p>You can run this scenario file agains the demo server:</p> <pre><code>lueur demo run\n</code></pre> <p>To execute the scenario file, run the following command:</p> <pre><code>lueur scenario run --scenario scenario.yaml\n</code></pre>"},{"location":"reference/scenario-file-format/#json-schema","title":"JSON Schema","text":"<p>Below is the full JSON schema of the scenario file:</p> scenario-schema.json<pre><code>{\n  \"$ref\": \"#/$defs/Scenario\",\n  \"$defs\": {\n    \"Scenario\": {\n      \"title\": \"Scenario\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"title\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"scenarios\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/$defs/ScenarioItem\"\n          }\n        }\n      },\n      \"required\": [\n        \"title\",\n        \"description\",\n        \"scenarios\"\n      ]\n    },\n    \"ScenarioItem\": {\n      \"title\": \"ScenarioItem\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"call\": {\n          \"$ref\": \"#/$defs/ScenarioItemCall\"\n        },\n        \"context\": {\n          \"$ref\": \"#/$defs/ScenarioItemContext\"\n        },\n        \"expect\": {\n          \"anyOf\": [\n            {\n              \"type\": \"null\"\n            },\n            {\n              \"$ref\": \"#/$defs/ScenarioItemExpectation\"\n            }\n          ]\n        }\n      },\n      \"required\": [\n        \"call\",\n        \"context\",\n        \"expect\"\n      ]\n    },\n    \"ScenarioItemCall\": {\n      \"title\": \"ScenarioItemCall\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"method\": {\n          \"type\": \"string\"\n        },\n        \"url\": {\n          \"type\": \"string\"\n        },\n        \"headers\": {\n          \"anyOf\": [\n            {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              }\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"body\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        }\n      },\n      \"required\": [\n        \"method\",\n        \"url\",\n        \"headers\",\n        \"body\"\n      ]\n    },\n    \"ScenarioItemContext\": {\n      \"title\": \"ScenarioItemContext\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"upstreams\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"faults\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/$defs/FaultConfiguration\"\n          }\n        },\n        \"strategy\": {\n          \"anyOf\": [\n            {\n              \"type\": \"null\"\n            },\n            {\n              \"$ref\": \"#/$defs/ScenarioItemCallStrategy\"\n            }\n          ]\n        }\n      },\n      \"required\": [\n        \"upstreams\",\n        \"faults\",\n        \"strategy\"\n      ]\n    },\n    \"FaultConfiguration\": {\n      \"title\": \"FaultConfiguration\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"Latency\": {\n          \"$ref\": \"#/$defs/Latency\"\n        },\n        \"PacketLoss\": {\n          \"$ref\": \"#/$defs/PacketLoss\"\n        },\n        \"Bandwidth\": {\n          \"$ref\": \"#/$defs/Bandwidth\"\n        },\n        \"Jitter\": {\n          \"$ref\": \"#/$defs/Jitter\"\n        },\n        \"Dns\": {\n          \"$ref\": \"#/$defs/Dns\"\n        },\n        \"HttpError\": {\n          \"$ref\": \"#/$defs/HttpError\"\n        }\n      },\n      \"required\": [\n        \"Latency\",\n        \"PacketLoss\",\n        \"Bandwidth\",\n        \"Jitter\",\n        \"Dns\",\n        \"HttpError\"\n      ]\n    },\n    \"Latency\": {\n      \"title\": \"Latency\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"distribution\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"global_\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"side\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"client\",\n                \"server\"\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"mean\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"stddev\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"min\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"max\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"shape\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"scale\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"direction\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"egress\",\n                \"ingress\"\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        }\n      },\n      \"required\": [\n        \"distribution\",\n        \"global_\",\n        \"side\",\n        \"mean\",\n        \"stddev\",\n        \"min\",\n        \"max\",\n        \"shape\",\n        \"scale\",\n        \"direction\"\n      ]\n    },\n    \"PacketLoss\": {\n      \"title\": \"PacketLoss\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"direction\": {\n          \"enum\": [\n            \"egress\",\n            \"ingress\"\n          ]\n        },\n        \"side\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"client\",\n                \"server\"\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        }\n      },\n      \"required\": [\n        \"direction\",\n        \"side\"\n      ]\n    },\n    \"Bandwidth\": {\n      \"title\": \"Bandwidth\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"rate\": {\n          \"type\": \"integer\",\n          \"minimum\": 0,\n          \"default\": 1000\n        },\n        \"unit\": {\n          \"enum\": [\n            \"bps\",\n            \"gbps\",\n            \"kbps\",\n            \"mbps\"\n          ],\n          \"default\": \"bps\"\n        },\n        \"direction\": {\n          \"enum\": [\n            \"egress\",\n            \"ingress\"\n          ],\n          \"default\": \"server\"\n        },\n        \"side\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"client\",\n                \"server\"\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": \"server\"\n        }\n      },\n      \"required\": []\n    },\n    \"Jitter\": {\n      \"title\": \"Jitter\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"side\": {\n          \"anyOf\": [\n            {\n              \"enum\": [\n                \"client\",\n                \"server\"\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"amplitude\": {\n          \"type\": \"number\",\n          \"minimum\": 0.0,\n          \"default\": 20.0\n        },\n        \"frequency\": {\n          \"type\": \"number\",\n          \"minimum\": 0.0,\n          \"default\": 5.0\n        }\n      },\n      \"required\": [\n        \"side\"\n      ]\n    },\n    \"Dns\": {\n      \"title\": \"Dns\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"rate\": {\n          \"type\": \"number\",\n          \"minimum\": 0.0,\n          \"maximum\": 1.0,\n          \"default\": 0.5\n        }\n      },\n      \"required\": []\n    },\n    \"HttpError\": {\n      \"title\": \"HttpError\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"body\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"status_code\": {\n          \"$ref\": \"#/$defs/HTTPStatus\",\n          \"default\": 500\n        },\n        \"probability\": {\n          \"type\": \"number\",\n          \"minimum\": 0.0,\n          \"maximum\": 1.0,\n          \"default\": 1.0\n        }\n      },\n      \"required\": [\n        \"body\"\n      ]\n    },\n    \"HTTPStatus\": {\n      \"title\": \"HTTPStatus\",\n      \"description\": \"HTTP status codes and reason phrases\\n\\n    Status codes from the following RFCs are all observed:\\n\\n        * RFC 7231: Hypertext Transfer Protocol (HTTP/1.1), obsoletes 2616\\n        * RFC 6585: Additional HTTP Status Codes\\n        * RFC 3229: Delta encoding in HTTP\\n        * RFC 4918: HTTP Extensions for WebDAV, obsoletes 2518\\n        * RFC 5842: Binding Extensions to WebDAV\\n        * RFC 7238: Permanent Redirect\\n        * RFC 2295: Transparent Content Negotiation in HTTP\\n        * RFC 2774: An HTTP Extension Framework\\n        * RFC 7725: An HTTP Status Code to Report Legal Obstacles\\n        * RFC 7540: Hypertext Transfer Protocol Version 2 (HTTP/2)\\n        * RFC 2324: Hyper Text Coffee Pot Control Protocol (HTCPCP/1.0)\\n        * RFC 8297: An HTTP Status Code for Indicating Hints\\n        * RFC 8470: Using Early Data in HTTP\",\n      \"enum\": [\n        100,\n        101,\n        102,\n        103,\n        200,\n        201,\n        202,\n        203,\n        204,\n        205,\n        206,\n        207,\n        208,\n        226,\n        300,\n        301,\n        302,\n        303,\n        304,\n        305,\n        307,\n        308,\n        400,\n        401,\n        402,\n        403,\n        404,\n        405,\n        406,\n        407,\n        408,\n        409,\n        410,\n        411,\n        412,\n        413,\n        414,\n        415,\n        416,\n        417,\n        418,\n        421,\n        422,\n        423,\n        424,\n        425,\n        426,\n        428,\n        429,\n        431,\n        451,\n        500,\n        501,\n        502,\n        503,\n        504,\n        505,\n        506,\n        507,\n        508,\n        510,\n        511\n      ]\n    },\n    \"ScenarioItemCallStrategy\": {\n      \"title\": \"ScenarioItemCallStrategy\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"mode\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/$defs/ScenarioItemCallStrategyMode\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"failfast\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"step\": {\n          \"type\": \"number\",\n          \"minimum\": 0.0\n        },\n        \"wait\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"add_baseline_call\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"count\": {\n          \"type\": \"integer\",\n          \"minimum\": 0,\n          \"default\": 0\n        }\n      },\n      \"required\": [\n        \"mode\",\n        \"failfast\",\n        \"step\",\n        \"wait\",\n        \"add_baseline_call\"\n      ]\n    },\n    \"ScenarioItemCallStrategyMode\": {\n      \"title\": \"ScenarioItemCallStrategyMode\",\n      \"enum\": [\n        1\n      ]\n    },\n    \"ScenarioItemExpectation\": {\n      \"title\": \"ScenarioItemExpectation\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"status\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\",\n              \"minimum\": 0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"response_time_under\": {\n          \"anyOf\": [\n            {\n              \"type\": \"number\",\n              \"minimum\": 0.0\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        }\n      },\n      \"required\": [\n        \"status\",\n        \"response_time_under\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"tutorials/create-scenario/","title":"Creating a Reliability Testing Scenario","text":""},{"location":"tutorials/create-scenario/#introduction","title":"Introduction","text":"<p>Context:  </p> <p>Modern applications are not running in isolation. Whether it is a distributed   file system, a database or a remote API, applications depend on network to   be reliable and fast.</p> <p>Understanding how an application reacts under network duress is prime to build   more resilient systems overall.</p> <p>Goal:</p> <p>By the end of this tutorial, you will:</p> <ul> <li>Configure Lueur to apply latency.</li> <li>Run a defined scenario that systematically applies this fault.</li> <li>Observe the application\u2019s behavior and interpret the resulting report.</li> </ul>"},{"location":"tutorials/create-scenario/#prerequisites","title":"Prerequisites","text":"<p>Tools &amp; Setup:</p> <ul> <li>Lueur installed on your local machine.</li> <li>An existing application or a simple test client that makes calls to a known     third-party endpoint (e.g., <code>https://api.example.com</code>).</li> <li>Basic familiarity with setting <code>HTTP_PROXY</code> or <code>HTTPS_PROXY</code> environment     variables.</li> </ul> <p>Assumptions:   The tutorial assumes you have followed the   Getting Started tutorial and understand how to launch   lueur proxy.</p>"},{"location":"tutorials/create-scenario/#step-1-choosing-the-third-party-endpoint","title":"Step 1: Choosing the Third-Party Endpoint","text":"<p>Before simulating any faults, it\u2019s essential to establish a reliable baseline. This step ensures that your application can communicate successfully with a stable API, so you know that any issues observed later are truly due to the injected faults.</p>"},{"location":"tutorials/create-scenario/#how-to-pick-a-stable-endpoint","title":"How to Pick a Stable Endpoint","text":"<ul> <li> <p>Reachability:   lueur supports HTTP/1.1 and HTTP/2 only. If your endpoint only responds to   HTTP/3, lueur cannot work with it.</p> </li> <li> <p>Consistency:   Select an endpoint known for its consistency. A public API that rarely experiences   downtime is ideal.</p> </li> <li> <p>Predictability:   The endpoint should return predictable responses, making it easier to spot the impact   of any simulated network faults.</p> </li> </ul> <p>For demonstration purposes, use <code>http://localhost:7070</code>.</p>"},{"location":"tutorials/create-scenario/#step-2-creating-a-scenario-file","title":"Step 2: Creating a Scenario File","text":"<p>In this step, you'll create a scenario file in YAML that defines a series of tests. Each scenario acts like a mini-test case, telling lueur exactly how to simulate network faults and what to expect from your application. This file is your blueprint for reliability engineering.</p> <p>Follow these steps to build your scenario file:</p>"},{"location":"tutorials/create-scenario/#define-user-centric-metadata","title":"Define User-Centric Metadata","text":"<ul> <li>Title:   Every scenario starts with a clear title. This gives you a quick reference for   what the test is about.</li> <li>Description:   Optionally, add a short description for extra context about the scenario.</li> </ul> <p>Example:</p> <pre><code>---\ntitle: \"Latency Increase By 30ms Steps From Downstream\"\ndescription: \"A collection of tests to evaluate how our service handles network faults.\"\n</code></pre>"},{"location":"tutorials/create-scenario/#define-a-scenario-test","title":"Define a Scenario Test","text":"<p>Each item in the scenarios array represents one test case. It must contain three parts:</p> <p>Call:</p> <p>This section defines the HTTP request that Lueur will make.</p> <ul> <li><code>method</code>: The HTTP method (GET, POST, etc.).</li> <li><code>url</code>: The full URL to call.</li> <li><code>headers</code>: An object with header key-value pairs (if needed).</li> <li><code>body</code>: The request payload (if needed).</li> </ul> <pre><code>call:\n  method: GET\n  url: http://localhost:7070/ping\n</code></pre> <p>Context:</p> <p>This section tells Lueur which upstream services are involved and which faults to inject.</p> <ul> <li><code>upstreams</code>: An array of endpoints (as strings) where faults should be applied.</li> <li><code>faults</code>: An array of fault configurations. The JSON schema defines the   structure for each fault type (Latency, PacketLoss, Bandwidth, etc.).</li> <li><code>strategy</code>: (Optional) Defines how to repeat the test with incremental changes   (for example, gradually increasing latency).</li> </ul> <pre><code>context:\n  upstreams:\n    - https://postman-echo.com\n  faults:\n    - type: latency\n      mean: 80\n      stddev: 5\n      direction: ingress\n      side: client\n  strategy:\n    mode: Repeat\n    step: 30\n    count: 3\n    add_baseline_call: true\n</code></pre> <p>The <code>add_baseline_call</code> property is useful when you want to make a first call to your application without applying any faults. This provides a very basic baseline record of your application in normal conditions.</p> <p>The test declares that traffic going to upstream <code>https://postman-echo.com</code> will be routed to the proxy and that latency will be applied to ingress traffic from this endpoint.</p> <p>Note</p> <p>The reasone we are using this server here is because the demo application provided by lueur makes a call to it when the <code>/ping</code> endpoint is called.</p> <p>Expect:</p> <p>This section specifies the criteria that determine whether the test has passed.</p> <p><code>status</code>: The expected HTTP status code (or null). <code>response_time_under</code>: The maximum allowed response time (in milliseconds).</p> <pre><code>expect:\n  status: 200\n  response_time_under: 490\n</code></pre> <p>Putting it all together:</p> <pre><code>---\ntitle: \"Latency Increase By 30ms Steps From Downstream\"\ndescription: \"A collection of tests to evaluate how our service handles network faults.\"\nscenarios:\n  - call:\n      method: GET\n      url: http://localhost:7070/ping\n    context:\n      upstreams:\n        - https://postman-echo.com\n      faults:\n        - type: latency\n          mean: 80\n          stddev: 5\n          direction: ingress\n          side: client\n      strategy:\n        mode: Repeat\n        step: 30\n        count: 3\n        add_baseline_call: true\n    expect:\n      status: 200\n      response_time_under: 490\n</code></pre>"},{"location":"tutorials/create-scenario/#step-3-configuring-your-application-and-environment","title":"Step 3: Configuring Your Application and Environment","text":"<p>Before running your fault injection scenarios, it's crucial to ensure that traffic to and from your application is routed via lueur's proxy.</p>"},{"location":"tutorials/create-scenario/#set-the-proxy-environment-variable","title":"Set the Proxy Environment Variable","text":"<p>Configure your environment so that all HTTPS traffic is routed through lueur. This is typically done by setting the <code>HTTP_PROXY</code>  and/or <code>HTTPS_PROXY</code> environment variable to point to lueur's proxy endpoint.</p> <ul> <li>On Linux/MacOS/Windows (WSL):</li> </ul> <pre><code>export HTTP_PROXY=http://127.0.0.1:8080\nexport HTTPS_PROXY=http://127.0.0.1:8080\n</code></pre> <ul> <li>On Windows:</li> </ul> <pre><code>set HTTP_PROXY=http://127.0.0.1:8080\nset HTTPS_PROXY=http://127.0.0.1:8080\n</code></pre> <p>or using Powershell:</p> <pre><code>$env:HTTP_PROXY = \"http://127.0.0.1:8080\"\n$env:HTTPS_PROXY = \"http://127.0.0.1:8080\"\n</code></pre>"},{"location":"tutorials/create-scenario/#step-4-running-the-scenario","title":"Step 4: Running the Scenario","text":"<p>Now that you\u2019ve defined your scenarios and configured your environment, it\u2019s time to run the tests and see lueur in action.</p>"},{"location":"tutorials/create-scenario/#run-the-scenario","title":"Run the Scenario","text":"<p>Execute the following command in your terminal:</p> <pre><code>lueur scenario run --scenario scenario.yaml\n</code></pre> <p>Tip</p> <p>You may pass a directory instead of a single file, lueur will process all of them as part of a single run.</p> <p>Here is the output of the run:</p> <pre><code>================ Running Scenarios ================\n\n\u280f  4/4  Latency Increase By 30ms Steps From Downstream  \u25ae\u25ae\u25ae\u25ae\n\n===================== Summary =====================\n\nTests run: 4, Tests failed: 1\nTotal time: 1.9s\n\nReport saved as report.json\n</code></pre> <p>Note</p> <p>We have 4 iterations even though we set the iteration count to <code>3</code> in the scenario. This is due to the fact we also added a baseline call first with the parameter <code>add_baseline_call: true</code>.</p>"},{"location":"tutorials/create-scenario/#whats-happening-behind-the-scenes","title":"What\u2019s Happening Behind the Scenes","text":"<p>Proxy Launch:</p> <ul> <li>lueur starts a local proxy server (by default at <code>http://127.0.0.1:8080</code>) to intercept and manipulate network traffic.</li> </ul> <p>Fault Injection:</p> <ul> <li>For each test defined in your scenario file, lueur applies the specified   network faults.</li> </ul> <p>Metrics and Logging:</p> <ul> <li>As the tests run, lueur captures detailed metrics (like response times,   status codes, and error occurrences) along with logs. All this data is then   saved to <code>scenario-report.json</code> for later analysis.</li> </ul>"},{"location":"tutorials/create-scenario/#step-5-observing-logs-and-output","title":"Step 5: Observing Logs and Output","text":"<p>lueur records metrics while running the scenario. You can use this information to analyse the way your application reacted to increasingly degraded network conditions.</p> <p>lueur produces two files:</p> <ul> <li><code>results.json</code> Represents the structured log of the scenario execution.   Notably, it shows the faults as they were applied</li> <li><code>report.json</code> Represents an automated analysis of the run. lueur applies some   heuristics to evaluate what would be the impact on a variety of service-level   objectives (SLO)</li> </ul>"},{"location":"tutorials/create-scenario/#run-metrics","title":"Run Metrics","text":"<p>Here is an example of <code>results.json</code> file:</p> <pre><code>{\n  \"plugins\": [],\n  \"items\": [\n    {\n      \"title\": \"Latency Increase By 30ms Steps From Downstream\",\n      \"timestamp\": 1741273041489713,\n      \"target\": {\n        \"address\": \"http://localhost:7070/ping\"\n      },\n      \"metrics\": {\n        \"dns\": [],\n        \"protocol\": {\n          \"type\": \"http\",\n          \"code\": 200,\n          \"body_length\": 308\n        },\n        \"ttfb\": 411.620809,\n        \"total_time\": 411.620792,\n        \"faults\": []\n      },\n      \"expect\": {\n        \"type\": \"http\",\n        \"wanted\": {\n          \"status_code\": 200,\n          \"response_time_under\": 490.0\n        },\n        \"got\": {\n          \"status_code\": 200,\n          \"response_time\": 411.620792,\n          \"decision\": \"success\"\n        }\n      },\n      \"faults\": [],\n      \"errors\": [],\n      \"total_time\": 0.0\n    },\n    {\n      \"title\": \"Latency Increase By 30ms Steps From Downstream\",\n      \"timestamp\": 1741273041972099,\n      \"target\": {\n        \"address\": \"http://localhost:7070/ping\"\n      },\n      \"metrics\": {\n        \"dns\": [],\n        \"protocol\": {\n          \"type\": \"http\",\n          \"code\": 200,\n          \"body_length\": 308\n        },\n        \"ttfb\": 478.129449,\n        \"total_time\": 478.129428,\n        \"faults\": []\n      },\n      \"expect\": {\n        \"type\": \"http\",\n        \"wanted\": {\n          \"status_code\": 200,\n          \"response_time_under\": 490.0\n        },\n        \"got\": {\n          \"status_code\": 200,\n          \"response_time\": 478.129428,\n          \"decision\": \"success\"\n        }\n      },\n      \"faults\": [\n        {\n          \"type\": \"latency\",\n          \"distribution\": null,\n          \"global\": null,\n          \"side\": \"client\",\n          \"mean\": 80.0,\n          \"stddev\": 5.0,\n          \"min\": null,\n          \"max\": null,\n          \"shape\": null,\n          \"scale\": null,\n          \"direction\": \"ingress\"\n        }\n      ],\n      \"errors\": [],\n      \"total_time\": 0.0\n    },\n    {\n      \"title\": \"Latency Increase By 30ms Steps From Downstream\",\n      \"timestamp\": 1741273042577946,\n      \"target\": {\n        \"address\": \"http://localhost:7070/ping\"\n      },\n      \"metrics\": {\n        \"dns\": [],\n        \"protocol\": {\n          \"type\": \"http\",\n          \"code\": 200,\n          \"body_length\": 307\n        },\n        \"ttfb\": 601.706611,\n        \"total_time\": 601.706599,\n        \"faults\": []\n      },\n      \"expect\": {\n        \"type\": \"http\",\n        \"wanted\": {\n          \"status_code\": 200,\n          \"response_time_under\": 490.0\n        },\n        \"got\": {\n          \"status_code\": 200,\n          \"response_time\": 601.706599,\n          \"decision\": \"failure\"\n        }\n      },\n      \"faults\": [\n        {\n          \"type\": \"latency\",\n          \"distribution\": null,\n          \"global\": null,\n          \"side\": \"client\",\n          \"mean\": 110.0,\n          \"stddev\": 5.0,\n          \"min\": null,\n          \"max\": null,\n          \"shape\": null,\n          \"scale\": null,\n          \"direction\": \"ingress\"\n        }\n      ],\n      \"errors\": [],\n      \"total_time\": 0.0\n    },\n    {\n      \"title\": \"Latency Increase By 30ms Steps From Downstream\",\n      \"timestamp\": 1741273043255539,\n      \"target\": {\n        \"address\": \"http://localhost:7070/ping\"\n      },\n      \"metrics\": {\n        \"dns\": [],\n        \"protocol\": {\n          \"type\": \"http\",\n          \"code\": 200,\n          \"body_length\": 308\n        },\n        \"ttfb\": 673.595708,\n        \"total_time\": 673.595687,\n        \"faults\": []\n      },\n      \"expect\": {\n        \"type\": \"http\",\n        \"wanted\": {\n          \"status_code\": 200,\n          \"response_time_under\": 490.0\n        },\n        \"got\": {\n          \"status_code\": 200,\n          \"response_time\": 673.595687,\n          \"decision\": \"failure\"\n        }\n      },\n      \"faults\": [\n        {\n          \"type\": \"latency\",\n          \"distribution\": null,\n          \"global\": null,\n          \"side\": \"client\",\n          \"mean\": 140.0,\n          \"stddev\": 5.0,\n          \"min\": null,\n          \"max\": null,\n          \"shape\": null,\n          \"scale\": null,\n          \"direction\": \"ingress\"\n        }\n      ],\n      \"errors\": [],\n      \"total_time\": 0.0\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/create-scenario/#report-analysis","title":"Report Analysis","text":"<p>lueur is able to generate a report for you when running the scenario. By default, it will serialize it to JSON. Alternatively, you may change this to YAML or Markdown. lueur will select the right format based on the extension of the report file. For instance, we could have executed the scenario as follows:</p> <pre><code>lueur scenario run --scenario scenario.yaml --report report.md\n</code></pre> <p>Here is an example its output:</p> <pre><code># Lueur Resilience Test Report\n\n| **Endpoint** | **Total Fault Injected** | **SLO: 99% &lt; 200ms** | **SLO: 95% &lt; 500ms** | **SLO: 90% &lt; 1s** | **SLO: 99% &lt; 1% Error Rate** | **SLO: 95% &lt; 0.5% Error Rate** |\n|-------------|--------------------------|-----------------------|-----------------------|-----------------------|----------------------------------|-----------------------------------|\n| `http://localhost:7070/ping` |  | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| `http://localhost:7070/ping` | - Latency Fault [Global: true, Side: client, Mean: 80.00 ms, Stddev: 5.00 ms, Direction: ingress] | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| `http://localhost:7070/ping` | - Latency Fault [Global: true, Side: client, Mean: 110.00 ms, Stddev: 5.00 ms, Direction: ingress] | \u274c (+401.7ms), 0h 0m 0.4s | \u274c (+101.7ms), 0h 0m 0.1s | \u274c (+-398.3ms), 0h 0m 0s | \u274c (+49.0%), 0h 0m 0.1s | \u274c (+49.5%), 0h 0m 0.1s |\n| `http://localhost:7070/ping` | - Latency Fault [Global: true, Side: client, Mean: 140.00 ms, Stddev: 5.00 ms, Direction: ingress] | \u274c (+473.6ms), 0h 0m 0.5s | \u274c (+173.6ms), 0h 0m 0.2s | \u274c (+-326.4ms), 0h 0m 0s | \u274c (+49.0%), 0h 0m 0.1s | \u274c (+49.5%), 0h 0m 0.1s |\n## Summary\n- **Total Test Cases:** 4\n- **Failures:** 2\n- \ud83d\udd0d Recommendation: Investigate the failed test cases to enhance your application's resilience.\n\n## Fault Type Analysis\n\n\n## Recommendations\n- No specific recommendations based on fault types.\n</code></pre> <p>Bring our own SLOs</p> <p>In a future release, lueur will allow you to pull information from your own SLO to give a more bespoke picture.</p>"},{"location":"tutorials/create-scenario/#step-6-identifying-areas-for-improvement","title":"Step 6: Identifying Areas for Improvement","text":"<p>Now that you\u2019ve run your scenarios, it\u2019s time to take a close look at the results and ask yourself: How did your application really perform under these simulated network conditions? Reflect on the following questions:</p> <p>Latency Handling:   Did your application gracefully manage the injected latency, or did some   requests time out?</p> <p>Error Handling and Retries:   Although these examples focus on latency, think about how your system would   respond to more disruptive faults\u2014like injected HTTP 500 errors. Are your   error-handling and retry mechanisms robust enough to recover gracefully?</p> <p>Bandwidth Constraints:   Consider how the application behaves under limited bandwidth scenarios.   Would a throttled connection significantly affect user experience or internal   performance?</p>"},{"location":"tutorials/create-scenario/#detailed-breakdown","title":"Detailed Breakdown","text":"<p>Test 1: Baseline Call (No Fault Injected)</p> <ul> <li>Response Time: 411.62ms</li> <li>Expected: Under 490ms</li> <li>Outcome: Success Your service handled the request quickly under ideal conditions.</li> </ul> <p>Test 2: Latency Fault with Mean 80ms</p> <ul> <li>Injected Fault: Latency fault with a mean of 80ms</li> <li>Response Time: 478.13ms</li> <li>Expected: Under 490ms</li> <li>Outcome: Success The slight increase in latency was within acceptable limits.</li> </ul> <p>Test 3: Latency Fault with Mean 110ms</p> <ul> <li>Injected Fault: Latency fault with a mean of 110ms</li> <li>Response Time: 601.71ms</li> <li>Expected: Under 490ms</li> <li>Outcome: Failure The additional latency caused the response time to exceed the target threshold.</li> </ul> <p>Test 4: Latency Fault with Mean 140ms</p> <ul> <li>Injected Fault: Latency fault with a mean of 140ms</li> <li>Response Time: 673.60ms</li> <li>Expected: Under 490ms</li> <li>Outcome: Failure The response time further degraded, confirming that higher latency critically impacts performance.</li> </ul>"},{"location":"tutorials/create-scenario/#interpreting-the-results","title":"Interpreting the Results","text":"<ul> <li> <p>Performance Sensitivity:   The baseline and initial fault test (80ms mean) indicate your application   performs well under slight latency. However, when the latency increases beyond   a certain point (110ms and 140ms), the response time quickly escalates,   leading to failures.</p> </li> <li> <p>Threshold Identification:   These results help you pinpoint the latency threshold where your application   begins to struggle. Knowing this, you can set realistic performance targets   and optimize system behavior for expected network conditions.</p> </li> <li> <p>Insight into Resilience:   The incremental steps in fault injection reveal exactly how your system's   performance degrades. This information is crucial for making targeted   improvements\u2014whether it\u2019s refining retry logic, adjusting timeouts, or   optimizing resource management.</p> </li> </ul>"},{"location":"tutorials/create-scenario/#next-steps-based-on-these-insights","title":"Next Steps Based on These Insights","text":"<ul> <li> <p>Investigate Bottlenecks:   Analyze why your service handles up to 80ms latency successfully but fails at   higher levels. This could be due to slow dependencies, inefficient error   handling, or suboptimal timeouts.</p> </li> <li> <p>Enhance Fault Tolerance:   Consider implementing circuit breakers or adaptive retry mechanisms that kick   in as latency increases.</p> </li> <li> <p>Iterate and Test:   Use these insights to further refine your scenarios. Adjust the fault   parameters and re-run tests to see if your improvements yield the desired   performance enhancements.</p> </li> </ul>"},{"location":"tutorials/create-scenario/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you learned how to:</p> <ul> <li> <p>Define and run a scenario:   You created a scenario file to simulate multiple network faults\u2014such as   latency, bandwidth constraints, and error injections\u2014on a real HTTP call.</p> </li> <li> <p>Observe real-world impact:   By running your scenarios, you observed how your application behaves under   stress. The collected metrics and logs provided clear evidence of its   strengths and weaknesses.</p> </li> <li> <p>Gather actionable data:   The insights from the test reports guided you in identifying areas for   performance optimization and error handling improvements.</p> </li> </ul> <p>By integrating these practices into your development cycle, you can catch issues earlier in the process\u2014ensuring that your application is more resilient and production-ready. This proactive approach not only improves overall system reliability but also paves the way for a smoother, more confident path to production.</p>"},{"location":"tutorials/create-scenario/#next-steps","title":"Next Steps","text":"<ul> <li>Point to other How-To guides for fine-tuning scenarios or integrating lueur into CI pipelines.</li> <li>Suggest expanding the scenario file with more complex tests or different endpoints.</li> <li>Encourage experimenting with different fault profiles to continuously challenge and improve the application\u2019s resilience.</li> </ul>"},{"location":"tutorials/getting-started/","title":"Getting Started with lueur","text":"<p>Welcome to lueur\u2014your new ally in exploring and understanding the impact of these petty network issues on your application! In this brief tutorial, we\u2019ll help you get up and running with lueur so that you can start experimenting with network faults and latency right from your own environment.</p> <p>By the end of this tutorial, you\u2019ll have:</p> <ul> <li>Installed lueur on your machine.</li> <li>Started a local proxy to simulate network conditions.</li> <li>Started a local demo application for learning purpose</li> <li>Made your first request through the proxy, observing how latency affects the   application.</li> </ul> <p>Let\u2019s get started!</p>"},{"location":"tutorials/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before diving in, make sure you have the following:</p> <ul> <li>A supported operating system: lueur runs smoothly on most modern Linux,   macOS, and Windows systems.</li> </ul>"},{"location":"tutorials/getting-started/#step-1-installation","title":"Step 1: Installation","text":"<p>If you haven\u2019t installed lueur yet, here\u2019s a quick way to do it:</p> <ol> <li>Head over to the lueur Releases page    and download the binary for your platform.  </li> <li>Extract the binary and place it in a directory included in your <code>$PATH</code>    (like <code>/usr/local/bin</code> on Linux/macOS or in a PATH-enabled directory on    Windows).</li> </ol>"},{"location":"tutorials/getting-started/#step-2-starting-the-local-proxy","title":"Step 2: Starting the Local Proxy","text":"<p>lueur operates by running a local proxy server. You can route your application\u2019s traffic through it to simulate network faults. Let\u2019s start a simple latency scenario:</p> <pre><code>lueur run --upstream http://localhost:7070 --with-latency --latency-mean 300\n</code></pre> <p>This command launches the lueur proxy on a local port (by default, <code>127.0.0.1:8080</code>) and injects an average of <code>300ms</code> latency into outgoing requests. You can adjust the <code>--latency-mean</code> value to experiment with different latencies.</p> <p>The <code>--upstream http://localhost:7070</code> argument tells lueur to only process traffic from and to this host.</p> <p>Failure</p> <p>Note, if you see an error with a mesage such as <code>Os { code: 98, kind: AddrInUse, message: \"Address already in use\" }</code>, it is a signe that another process is listening on the same address.</p> <p>Tip</p> <p>Always remember to set the right upstream server address that matches the endpoints you are exploring. You can set many <code>--upstream</code> arguments.</p> <p>Any traffic received by lueur that does not match any of these upstream addresses will go through the proxy unaltered.</p> <p>Once started, the proxy should issue the following message:</p> <pre><code>Welcome to lueur \u2014 Your Resiliency Exploration Tool!\n\nTo get started, route your HTTP/HTTPS requests through:\nhttp://127.0.0.1:8080\n\nAs you send requests, lueur will simulate network conditions\nso you can see how your application copes.\n\nReady when you are \u2014 go ahead and make some requests!\n\n\nConfigured faults:\n  - Latency: per packet: false, side: Server, direction: Ingress, distribution: Normal, mean: 300ms\n\nHosts Covered By The Faults:\n  - localhost:7070\n</code></pre> <p>Notice how the output tells you the address of the proxy server to use from your clients.</p> <p>You are now ready to roll!</p>"},{"location":"tutorials/getting-started/#step-3-starting-a-demo-application","title":"Step 3: Starting a demo application","text":"<p>For the purpose of this tutorial, we will use a demo application built-in into lueur.</p> <p>Start the demo application in a different terminal:</p> <pre><code>lueur demo run\n</code></pre> <p>This will start an application and listen for HTTP requests on <code>http://localhost:7070</code>.</p> <p>This will output the following prelude:</p> <pre><code>Welcome to lueur, this demo application is here to let you explore lueur's capabilities.\n\nHere are a few examples:\n\nexport HTTP_PROXY=http://localhost:8080\nexport HTTPS_PROXY=http://localhost:8080\n\ncurl -x ${HTTP_PROXY} http://127.0.0.1:7070/\ncurl -x ${HTTP_PROXY} http://127.0.0.1:7070/ping\ncurl -x ${HTTP_PROXY} http://127.0.0.1:7070/ping/myself\ncurl -x ${HTTP_PROXY} --json '{\"content\": \"hello\"}' http://127.0.0.1:7070/uppercase\n</code></pre> <p>The demo describes which endpoints are available and how to call them.</p> <p>First, you can verify the demo is running correctly with <code>curl</code>:</p> <pre><code>curl http://localhost:7070\n</code></pre> <p>which should output:</p> <pre><code>&lt;h1&gt;Hello, World!&lt;/h1&gt;\n</code></pre> <p>Look at the demo application output and you should see the request was served:</p> <pre><code>GET / 200 6.627\u00b5s\n</code></pre> <p>The given timing <code>6.627\u00b5s</code> represents the duration of the request/response processing by the demo application for that particular request.</p> <p>Let's now enrich the <code>curl</code> command above to output the time taken from the client's perspective:</p> <pre><code>curl -I -o /dev/null -s \\\n  -w \"Connected IP: %{remote_ip}:%{remote_port}\\nTotal time: %{time_total}s\\n\" \\\n  http://localhost:7070\n</code></pre> <p>This should display something such as:</p> <pre><code>Connected IP: 127.0.0.1:7070\nTotal time: 0.000239s\n</code></pre> <p>The time is displayed in seconds. Here the response took <code>239\u00b5s</code>.</p> <p>Let's now move to the next stage, inducing latency impacting the client's point of view of the time taken to receive a response from the demo application.</p>"},{"location":"tutorials/getting-started/#step-4-configuring-your-application-to-use-the-proxy","title":"Step 4: Configuring Your Application to Use the Proxy","text":"<p>Now that lueur\u2019s running, configure your application\u2019s HTTP requests to pass through the proxy.</p> <p>For example, if you\u2019re using <code>curl</code>, you might do:</p> <pre><code>curl -I -o /dev/null -s \\\n  -w \"Connected IP: %{remote_ip}:%{remote_port}\\nTotal time: %{time_total}s\\n\" \\\n  -x http://127.0.0.1:8080 \\\n  http://localhost:7070\n</code></pre> <p>With <code>-x http://127.0.0.1:8080</code> set, all requests made via <code>curl</code> will flow through lueur, experiencing the specified latency. By observing your application\u2019s behavior (whether it\u2019s a command-line tool, a local service, or a browser hitting a test endpoint), you\u2019ll gain first-hand insight into how network slowdowns affect it.</p> <p>Tip</p> <p>Most of the time, you can set either the <code>HTTP_PROXY</code> or <code>HTTPS_PROXY</code> environment variables to let your client know it needs to go through a proxy: <code>export HTTP_PROXY=http://127.0.0.1:8080</code>.</p> <p>Once you have executed that command, you should see a much higher response time:</p> <pre><code>Connected IP: 127.0.0.1:8080\nTotal time: 0.333350s\n</code></pre> <p>We are now above the <code>300ms</code> mark as per the configuration of our proxy.</p> <p>Fantastic, you have now succeeded in altering the perception your clients would have from your using your application. The only question remaining is whether or not this is a level that is acceptable by the organisation.</p>"},{"location":"tutorials/getting-started/#step-5-observing-the-effects","title":"Step 5: Observing the Effects","text":"<p>Trigger a few requests from your application. Notice how responses now arrive slightly delayed. This delay simulates real-world network conditions\u2014exactly what lueur is here to help you understand and address.</p> <ul> <li>If your application times out or behaves strangely under these conditions,   you\u2019ve just uncovered a resilience gap.</li> <li>If it gracefully handles delayed responses, congratulations! Your software   is a step closer to being truly reliable.</li> </ul>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<p>You\u2019ve successfully set up lueur, run your first latency scenario, and routed traffic through it. What\u2019s next?</p> <ul> <li>Try different latency values or other fault injection parameters to get   a feel for how your application responds to varied conditions.</li> <li>Explore our Scenario Tutorial to learn how to   simulate scenarios using files and generate detailed reports.</li> <li>Dive into How-To Guides to integrate lueur deeper into   your workflow, from automated testing to continuous integration.</li> </ul> <p>With this initial setup under your belt, you\u2019re well on your way to embracing a culture of resilience in your everyday development tasks. Happy experimenting!</p>"},{"location":"tutorials/install/","title":"Install lueur","text":"<p>lueur strives to get of your way and it starts with a smooth installation.</p>"},{"location":"tutorials/install/#download-lueur","title":"Download lueur","text":"<p>lueur is provided as a binary targetting the three major platforms: Linux, macOS and Windows.</p> <p>You can download the appropriate binary for your platform from here.</p> Configure the binary <p>Once you have downloaded the archive, you can uncompress it and make sure it can be found in your <code>PATH</code>.</p> Linux, macOS, Windows BashWindows Powershell <pre><code>export PATH=$PATH:`pwd`\n</code></pre> <p>Make also sure the downloaded binary is an executable:</p> <pre><code>chmod a+x lueur\n</code></pre> <pre><code>$env:Path += ';C:\\directoy\\where\\lueur\\lives' \n</code></pre> <p>Do you want to know more?</p> <p>Find more installation options here.</p>"},{"location":"tutorials/install/#check-lueur-is-ready-to-roll","title":"Check lueur is ready to roll","text":"<p>Let's verify it all went well by running the following command:</p> <pre><code>lueur --help\n</code></pre> <p>This should output the following:</p> <pre><code>A proxy to test network resilience by injecting various faults.\n\nUsage: lueur [OPTIONS] &lt;COMMAND&gt;\n\nCommands:\n  run       Run the lueur proxy and apply network faults to traffic\n  scenario  Execute a scenario\n  demo      Run a simple demo server for learning purpose\n  help      Print this message or the help of the given subcommand(s)\n\nOptions:\n      --log-file &lt;LOG_FILE&gt;    Path to the log file. Disabled by default\n      --log-stdout             Stdout logging enabled\n      --log-level &lt;LOG_LEVEL&gt;  Log level [default: info,tower_http=debug]\n      --with-otel              Enable open telemetry\n  -h, --help                   Print help\n  -V, --version                Print version\n</code></pre>"},{"location":"tutorials/install/#troubleshooting","title":"Troubleshooting","text":"<p>If you receive a message such as \u0300<code>lueur: No such file or directory</code>, it likely means you have not put the directory containing the <code>lueur</code> binary in your  <code>PATH</code>, or you may need to restart your session for the changes to take effect.</p>"},{"location":"tutorials/install/#next-steps","title":"Next Steps","text":"<p>You\u2019ve successfully downloaded and made lueure. What\u2019s next?</p> <ul> <li>Explore our Getting Started Tutorial to learn how to first use lueur.</li> <li>Dive into How-To Guides to integrate lueur deeper into   your workflow, from automated testing to continuous integration.</li> </ul>"}]}